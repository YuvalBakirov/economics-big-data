---
title: "big_data_final_task"
author: "Ariel&Eitan&Yuval"
date: "2024-10-05"
output: html_document
---

# Project: Predict Uber Demand in New York

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load necessary libraries
```{r necessary_libraries}
library(tidyr)
library(dplyr)
library(geosphere) 
library(leaflet)
library(ggplot2)
library(lubridate)
library(readr)
library(stringr)
library(corrplot)
library(car)
library(gridExtra)
library(grid)
library(scales)
library(zoo)
```

## Load the CSV files
```{r load_data}
train_raw_data <- read.csv("train_raw_data.csv")
train_2000m_data <- read.csv("train_raw_data_dists_more_then_2000.csv")
test_data <- read.csv("test_set.csv")
```

### train_raw_data.csv
train_raw_data.csv: Contains all Uber pickups in NYC between April 1 and September 9, 2014, without filtering by distance.

```{r raw_data_look}
# Show the first 5 rows of each train_raw_data
head(train_raw_data, 5)

# Summary statistics for the full dataset
summary(train_raw_data)
```
* Latitude (lat) ranges from 39.66 to 42.12, and Longitude (lon) ranges from -74.93 to -72.07, indicating the pickups occurred within the New York area.

* The median and mean values for both latitude (around 40.74) and longitude (around -73.97) show that most pickups took place near the central part of New York, close to the Empire State Building.

### train_raw_data_dists_more_then_2000.csv
train_raw_data_dists_more_then_2000.csv: Contains only Uber pickups outside a 2000-meter radius from the Empire State Building.

```{r filtered_data_look}
# Show the first 5 rows of each train_raw_data
head(train_2000m_data, 5)

# Summary statistics for the filtered dataset
summary(train_2000m_data)
```

* Similar latitude and longitude ranges, but these pickups occurred outside a 2000-meter radius from the Empire State Building.

* The dist column represents the distance from the Empire State Building, ranging from 2000 meters to 220,970 meters (about 221 km). The median distance is 4083 meters.

#### Features Explanation:
Datasets train_raw_data.csv and train_raw_data_dists_more_then_2000.csv features:

1. datetime: This column represents the date and time when the Uber pickup occurred.

2. lat: This column contains the latitude of the pickup location, indicating the geographical position (north-south axis) of the ride.

3. lon: This column contains the longitude of the pickup location, representing the geographical position (east-west axis) of the ride.

4. Base:  This column includes a code representing the TLC (Taxi and Limousine Commission) base company code affiliated with the Uber pickup. This code identifies the licensed dispatching base that managed the Uber trip. Every ride must be affiliated with a TLC-licensed base, which is responsible for dispatching the trip.(License Number).

5. dist (only for train_raw_data_dists_more_then_2000.csv): This column represents the distance (in meters) between the pickup location and the Empire State Building. All values in this dataset are greater than 2000 meters, as the dataset is filtered to include only pickups beyond this radius.

### test_set.csv
```{r}
test_data
```
We can observe that the test file contains time intervals in 15-minute increments, currently stored in character format. We need to convert these time intervals to POSIXct format for further analysis. The objective is then to use this data to predict the number of pickups using a trained model.

```{r}
test_fixed_data <- test_data %>%
  mutate(time_interval = as.POSIXct(time_interval, format="%Y-%m-%dT%H:%M:%SZ", tz = "UTC"))

test_fixed_data
```
Note: In order to predict the number_of_pickups using a ML model, our test data should have the same features that were used to train the model.So, it is crucial to ensure that all the transformations, feature engineering steps, creating new features and any external data applied during the training process are also applied to the test data. Ensuring consistency in features allows the model to process the test data accurately and make reliable predictions based on the learned relationships during training.

Looking at the map:
```{r interactive_map}
# # Define latitude, longitude, and radius
# lat <- 40.7484 # Empire State Building coordinates
# lon <- -73.985 # Empire State Building coordinates
# 
# small_radius <- 1000  # Radius in meters
# big_radius <- 2000  # Radius in meters
# 
# # Create the leaflet map
# leaflet() %>%
#   addTiles() %>%  # Add default OpenStreetMap tiles
#   addMarkers(lng = lon, lat = lat, popup = "Empire State Building") %>%  # 
#   addCircles(lng = lon, lat = lat, radius = small_radius, color = "blue", fillOpacity = 0.2) %>% # 
#   addCircles(lng = lon, lat = lat, radius = big_radius, color = "red", fillOpacity = 0.1)  #
```

## Part A: Data Rearrangement

### Filtering Data
Only relevant for the train_raw_data.csv data:
```{r}
# Define Empire State Building coordinates
empire_state_coords <- c(-73.985, 40.7484)  # Longitude, Latitude

# Calculate the distance from each pickup to the Empire State Building
train_raw_data <- train_raw_data %>%
  mutate(dist = distHaversine(cbind(lon, lat), empire_state_coords))
```

We would like to see how our data is distributed:
```{r}
# Add a new column 'distance_category' to classify the distance
train_raw_data <- train_raw_data %>%
  mutate(distance_category = ifelse(dist <= 1000, "<= 1000 meters", "> 1000 meters"))

ggplot(train_raw_data, aes(x = distance_category)) +
  geom_bar(fill = c("lightblue", "lightgreen")) +
  labs(title = "Count of Uber Pickups within and beyond 1000 meters",
       x = "Distance Category",
       y = "Count of Pickups") +
  theme_minimal()
```

We can see that there are many Uber pickups occurred beyond the 1000-meter radius that need to be filtered.

```{r filtering_data}
# Filter data to include only pickups within a 1000-meter radius and remove the 'distance_category' column
train_filtered_1000m_data <- train_raw_data %>%
  filter(dist <= 1000) %>%
  select(-distance_category)  # Remove the 'distance_category' column if it exists
```

Making sure the filter is done
```{r}
# Check the maximum distance in the filtered dataset
max_distance <- max(train_filtered_1000m_data$dist)

# Print the maximum distance
print(paste("Maximum distance in the 1000m dataset is:", max_distance))
```
Let's check how many rows in this dataset we are left with:
```{r}
rows_1000m <- nrow(train_filtered_1000m_data)
print(paste("Number of rows in the 1000m dataset:", rows_1000m))

rows_2000m <- nrow(train_2000m_data)
print(paste("Number of rows in the 2000m dataset:", rows_2000m))
```

```{r}
train_filtered_1000m_data
```

```{r}
train_2000m_data
```

Looking at the max distance of the 2000m data:
```{r}
# Check the maximum distance in the filtered dataset
max_distance_2000 <- max(train_2000m_data$dist)

# Print the maximum distance
print(paste("Maximum distance in the 2000m dataset is:", max_distance_2000))
```

### Filtering by Time
We need to make sure the timestamp column is formatted correctly.
* The timestamp format (for example: 2014-04-01T00:02:00Z) includes a "T" between the date and time and ends with a "Z", which indicates that the time is in UTC (Coordinated Universal Time).
We need to adjust the format argument to handle this format. Additionally, we can specify the time zone using tz = "UTC" to make sure the conversion handles the UTC time correctly.
```{r}
# Ensure the 'timestamp' column is in POSIXct format, handling the 'T' and 'Z' characters
train_filtered_1000m_data <- train_filtered_1000m_data %>%
  mutate(timestamp = as.POSIXct(timestamp, format="%Y-%m-%dT%H:%M:%SZ", tz = "UTC"))

train_filtered_2000m_data <- train_2000m_data %>%
  mutate(timestamp = as.POSIXct(timestamp, format="%Y-%m-%dT%H:%M:%SZ", tz = "UTC"))
```

Visualize the distribution of Uber pickups by hour to check if there are any outside 17:00-00:00:
```{r}
plot_pickup_distribution <- function(data, title, color) {
  ggplot(data, aes(x = hour(timestamp))) +
    geom_bar(fill = color) +
    labs(title = title,
         x = "Hour of Day",
         y = "Count of Pickups") +
    scale_x_continuous(breaks = 0:23) +  # Set x-axis for hours (0 to 23)
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for clarity
}
```
Lets say that each dataset will get a color: lightblue for the 1000m data and lightgreen for the 2000m data

```{r}
# For 1000-meter filtered data
plot_pickup_distribution(train_filtered_1000m_data, "Distribution of Uber Pickups by Hour (1000m Data)", "lightblue")
```

```{r}
# For 2000-meter filtered data
plot_pickup_distribution(train_filtered_2000m_data, "Distribution of Uber Pickups by Hour (2000m Data)", "lightgreen")
```
As seen in the two graphs, there are significant amounts of data outside the desired range of 17:00 to 00:00 for both datasets (1000m and 2000m data). Therefore, to focus on the data for pickups that occurred between 17:00 and 00:00, we will apply a filter to exclude the hours outside this range.

Filter data to include only pickups between 17:00 and 00:00:
```{r}
# Function to filter data between specified date-time range and specific hours (17:00 to 00:00)
filter_time_interval <- function(data, timestamp_column, start_datetime, end_datetime) {
  
  # Convert the timestamp column to a proper datetime format if needed
  data[[timestamp_column]] <- as.POSIXct(data[[timestamp_column]], tz = "UTC")
  
  # Apply the filter for the time range and desired hours
  filtered_data <- data %>%
    filter(
      !!sym(timestamp_column) >= as.POSIXct(start_datetime, tz = "UTC") &
      !!sym(timestamp_column) <= as.POSIXct(end_datetime, tz = "UTC") &
      (hour(!!sym(timestamp_column)) >= 17 & hour(!!sym(timestamp_column)) <= 23 | 
      (hour(!!sym(timestamp_column)) == 0 & minute(!!sym(timestamp_column)) == 0 & second(!!sym(timestamp_column)) == 0))
    )
  
  return(filtered_data)
}
```


```{r}
# For 1000-meter filtered data
train_filtered_1000m_data <- filter_time_interval(
  data = train_filtered_1000m_data,
  timestamp_column = "timestamp",
  start_datetime = "2014-04-01 17:00:00",
  end_datetime = "2014-09-09 00:00:00"
)

# For 2000-meter filtered data
train_filtered_2000m_data <- filter_time_interval(
  data = train_filtered_2000m_data,
  timestamp_column = "timestamp",
  start_datetime = "2014-04-01 17:00:00",
  end_datetime = "2014-09-09 00:00:00"
)
```

Check the Results:
```{r}
plot_hourly_pickup_distribution <- function(data, title, color) {
  # Create a data frame with all hours from 0 to 23
  hours <- data.frame(hour = 0:23)
  
  # Extract the hour and count the number of pickups per hour
  hourly_data <- data %>%
    mutate(hour = hour(timestamp)) %>%
    count(hour)
  
  # Merge the hourly counts with the 'hours' data frame to include missing hours as 0
  hourly_data_complete <- merge(hours, hourly_data, by = "hour", all.x = TRUE)
  hourly_data_complete[is.na(hourly_data_complete)] <- 0  # Replace NAs with 0
  
  # Plot the hourly data
  ggplot(hourly_data_complete, aes(x = hour, y = n)) +
    geom_bar(stat = "identity", fill = color) +
    labs(title = title,
         x = "Hour of Day",
         y = "Count of Pickups") +
    scale_x_continuous(breaks = 0:23) +  # Show all hours from 0 to 23 on x-axis
    theme_minimal()
}
```

```{r}
# Plot for 1000-meter filtered data (lightblue color)
plot_hourly_pickup_distribution(train_filtered_1000m_data, 
                                "Filtered Uber Pickups by Hour (1000m Data, Expected: 17:00 to 00:00)", 
                                "lightblue")
```
```{r}
# Plot for 2000-meter filtered data (lightgreen color)
plot_hourly_pickup_distribution(train_filtered_2000m_data, 
                                "Filtered Uber Pickups by Hour (2000m Data, Expected: 17:00 to 00:00)", 
                                "lightgreen")
```
We are left with only rows with the desired time range!

### Creating 15-Minute Intervals
We're grouping the `timestamp` data into 15-minute intervals to make it easier to spot patterns and trends in Uber pickups. This lets us count how many pickups happen in each time block, helping us better understand demand and predict future activity.

This action is critical because our test data is already divided into 15-minute intervals, so we need to apply the same approach to our training data to ensure consistency. By creating the number_of_pickups feature (the label), we align both datasets in terms of granularity, enabling the model to learn from the training data and make accurate predictions on the test data.

The function takes a dataset as input, creates the 15-minute time intervals, and calculates the pickup counts per interval:
```{r}
create_pickup_counts <- function(data) {
  pickup_counts <- data %>%
    mutate(time_interval = floor_date(timestamp, "15 minutes")) %>%
    group_by(time_interval) %>%
    summarise(number_of_pickups = n())

  return(pickup_counts)
}
```

```{r}
# Use the function for 1000m data
pickup_counts_1000m <- create_pickup_counts(train_filtered_1000m_data)
pickup_counts_1000m
```

```{r}
# Use the function for 2000m data
pickup_counts_2000m <- create_pickup_counts(train_filtered_2000m_data)
pickup_counts_2000m
```

```{r time_intervals}
create_time_intervals <- function(data, pickup_counts) {
  # Join the pickup counts back to the original data to include all columns
  data_with_intervals <- data %>%
    mutate(time_interval = floor_date(timestamp, "15 minutes")) %>%
    left_join(pickup_counts, by = "time_interval") %>%
    mutate(time_interval = as.POSIXct(time_interval, format = "%Y-%m-%d %H:%M:%S", tz = "UTC"))
  
  return(data_with_intervals)
}
```

```{r}
# Apply the function to the 1000-meter filtered data
train_filtered_1000m_data <- create_time_intervals(train_filtered_1000m_data, pickup_counts_1000m)

train_filtered_1000m_data
```

```{r}
# Apply the function to the 2000-meter filtered data
train_filtered_2000m_data <- create_time_intervals(train_filtered_2000m_data, pickup_counts_2000m)

train_filtered_2000m_data
```

Visualization that shows top 10 pickup counts for both the 1000-meter and 2000-meter filtered data:
```{r}
plot_top_10_pickup_intervals <- function(data, title, color) {
  # Arrange the data to get the top 10 intervals with the highest pickup counts
  top_10_pickups <- data %>%
    arrange(desc(number_of_pickups)) %>%
    head(10)
  
  # Convert time_interval to a character with both date and time format
  top_10_pickups <- top_10_pickups %>%
    mutate(time_interval = format(time_interval, "%Y-%m-%d %H:%M"))
  
  # Plot the top 10 intervals
  ggplot(top_10_pickups, aes(x = time_interval, y = number_of_pickups)) +
    geom_bar(stat = "identity", fill = color, width = 0.6) + # Adjust width for spacing
    labs(title = title,
         x = "Time Interval",
         y = "Pickup Counts") +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, size = 10), # Adjust text angle and size
      axis.text.y = element_text(size = 10),
      plot.margin = margin(t = 10, r = 10, b = 50, l = 10)          # Increase bottom margin
    ) +
    scale_x_discrete(labels = function(x) str_wrap(x, width = 10))  # Wrap the x-axis labels for better spacing
}
```

```{r}
# Plot for 1000-meter filtered data (lightblue color)
plot_top_10_pickup_intervals(pickup_counts_1000m, "Top 10 Pickup Intervals (1000m Radius)", "lightblue")
```
1000-meter Radius:

* The pickup counts in the top 10 intervals are around 150-175 pickups per interval.
* The busiest intervals span across different dates, showing a peak around July 15, 2014, with pickups happening consistently between 17:45 and 18:30 on several days.
* This indicates that Uber demand was high around these specific dates and times near the Empire State Building within a 1000-meter radius.

```{r}
# Plot for 2000-meter filtered data (lightgreen color)
plot_top_10_pickup_intervals(pickup_counts_2000m, "Top 10 Pickup Intervals (2000m Radius)", "lightgreen")
```
2000-meter Radius:

* The pickup counts in the top 10 intervals are significantly higher, around 500 pickups per interval.
* The busiest intervals are mostly concentrated on September 6, 2014, with intervals spanning between 17:15 and 23:30.
* This shows that the Uber demand beyond a 2000-meter radius from the Empire State Building was much higher on that specific date, indicating a spike in activity.

### Data Cleaning

#### Dealing with NAs
Check for NA in the datasets:
```{r}
# Function to count NAs for each column in the dataset
count_nas_per_column <- function(data) {
  na_counts <- sapply(data, function(col) sum(is.na(col)))
  return(na_counts)
}
```

```{r}
# For 1000m data
nas_1000m <- count_nas_per_column(train_filtered_1000m_data)
print("NA counts for 1000m data:")
print(nas_1000m)
```

```{r}
# For 2000m data
nas_2000m <- count_nas_per_column(train_filtered_2000m_data)
print("NA counts for 2000m data:")
print(nas_2000m)

```

```{r NAs}
# Check for NA values in the 1000m data
na_count_1000m <- sum(is.na(train_filtered_1000m_data))
print(paste("Number of NA values in 1000m data:", na_count_1000m))

# Check for NA values in the 2000m data
na_count_2000m <- sum(is.na(train_filtered_2000m_data))
print(paste("Number of NA values in 2000m data:", na_count_2000m))
```
There is no NAs in both of the filtered data sets!

#### Dealing with errors in the data
Check that there are no errors in timestamps (like invalid days, months, hours, minutes, or seconds):
```{r errors}
check_invalid_timestamps <- function(data) {
  # Extract the individual components from the timestamp and check for invalid values
  invalid_dates <- data %>%
    mutate(
      year = year(timestamp),
      month = month(timestamp),
      day = day(timestamp),
      hour = hour(timestamp),
      minute = minute(timestamp),
      second = second(timestamp)
    ) %>%
    filter(month > 12 | day > 31 | hour > 23 | minute > 59 | second > 59)
  
  # Print the results
  if (nrow(invalid_dates) > 0) {
    print("Invalid timestamps found:")
    print(invalid_dates)
  } else {
    print("All timestamps are valid.")
  }
}
```

```{r}
# Check for invalid timestamps in the 1000m data
cat("For 1000m data:\n")
check_invalid_timestamps(train_filtered_1000m_data)
```

```{r}
# Check for invalid timestamps in the 2000m data
cat("For 2000m data:\n")
check_invalid_timestamps(train_filtered_2000m_data)
```

Check if all the dates in the timestamp column are within the correct range (April 1, 2014, to September 9, 2014):
When all timestamps are within the correct range - it means that the range of the time intervals is correct as well because it based on it

```{r}
# Function to check if timestamps are within a valid range
check_date_range <- function(data, start_date, end_date) {
  # Filter for invalid dates outside the specified range
  invalid_dates <- data %>%
    filter(timestamp < start_date | timestamp > end_date)
  
  # Check and print results
  if (nrow(invalid_dates) > 0) {
    cat("Invalid timestamps found:\n")
    print(invalid_dates)
  } else {
    cat("All timestamps are within the correct range.\n")
  }
}
```

```{r}
# Define the valid date range
start_date <- as.POSIXct("2014-04-01 00:00:00", tz = "UTC")
end_date <- as.POSIXct("2014-09-09 23:59:59", tz = "UTC")
```

```{r}
# Check for the 1000m data
cat("For 1000m data:\n")
check_date_range(train_filtered_1000m_data, start_date, end_date)
```

```{r}
# Check for the 2000m data
cat("For 2000m data:\n")
check_date_range(train_filtered_2000m_data, start_date, end_date)

```

```{r}
# Check the range of the time_interval in the 1000m data
range(train_filtered_1000m_data$timestamp)
```

```{r}
# Check the range of the time_interval in the 2000m data
range(train_filtered_2000m_data$timestamp)
```

Check for errors in Latitude (lat) & Longitude (lon):

1. Latitude values should range between -90 and 90 (since latitude represents how far north or south a location is from the equator).

2.Longitude values should range between -180 and 180 (since longitude represents how far east or west a location is from the prime meridian).

```{r}
# Function to check for errors in 'lat', 'lon', and 'base'
check_lat_lon <- function(data) {
  # Filter for invalid latitude, longitude, or missing base
  invalid_data <- data %>%
    filter(lat < -90 | lat > 90 | 
           lon < -180 | lon > 180)
  
  # Check and print results
  if (nrow(invalid_data) > 0) {
    cat("Invalid latitude or longitude:\n")
    print(invalid_data)
  } else {
    cat("All lat & lon values are valid.\n")
  }
}
```

```{r}
# Check for the 1000m data
cat("For 1000m data:\n")
check_lat_lon(train_filtered_1000m_data)
```

```{r}
# Check for the 2000m data
cat("For 2000m data:\n")
check_lat_lon(train_filtered_2000m_data)
```

Check for errors in Base code:
Function to list all unique bases:
```{r}
list_unique_bases <- function(data) {
  unique_bases <- unique(data$base)
  return(unique_bases)
}
```

```{r}
# List unique bases in the 1000m dataset
unique_bases_1000m <- list_unique_bases(train_filtered_1000m_data)
print("Unique base codes in the 1000m dataset:")
print(unique_bases_1000m)
```

```{r}
# List unique bases in the 2000m dataset
unique_bases_2000m <- list_unique_bases(train_filtered_2000m_data)
print("Unique base codes in the 2000m dataset:")
print(unique_bases_2000m)
```

```{r}
# Function to compare unique bases between two datasets
compare_unique_bases <- function(bases_1000m, bases_2000m) {
  
  # Check if both sets have the same items
  same_bases <- setequal(bases_1000m, bases_2000m)
  
  if (same_bases) {
    print("Both 1000m and 2000m datasets have the same unique bases.")
  } else {
    print("The 1000m and 2000m datasets have different unique bases.")
    
    # Identify the bases that are in 1000m but not in 2000m
    diff_1000m <- setdiff(bases_1000m, bases_2000m)
    if (length(diff_1000m) > 0) {
      print("Bases present in 1000m but not in 2000m:")
      print(diff_1000m)
    }
    
    # Identify the bases that are in 2000m but not in 1000m
    diff_2000m <- setdiff(bases_2000m, bases_1000m)
    if (length(diff_2000m) > 0) {
      print("Bases present in 2000m but not in 1000m:")
      print(diff_2000m)
    }
  }
}

# Run the function to compare unique bases
compare_unique_bases(unique_bases_1000m, unique_bases_2000m)
```
We used: https://www.nyc.gov/assets/tlc/downloads/pdf/trip_record_user_guide.pdf to find the Base Name:
B02598 - HINTER LLC 
B02617 - WEITER LLC
B02682 - SCHMECKEN LLC
B02764 - DANACH-NY,LLC
B02512 - UNTER LLC

* It appears that all the base codes in the dataset are properly identified and correspond to valid base names, suggesting that there are no meaningless or invalid codes. Each base code is linked to a registered entity.

* The base names seem to be associated with different companies, but the names themselves may not directly provide information about the service type or location.

Lets look at the bases in everyday:
```{r}
# Function to count and plot the number of rows for each base, with labels on top of each bar
plot_base_counts <- function(data, title, fill_color) {
  # Count the number of rows for each base
  base_counts <- data %>%
    group_by(base) %>%
    summarise(num_rows = n()) %>%
    arrange(desc(num_rows))
  
  # Create the bar plot
  ggplot(base_counts, aes(x = reorder(base, -num_rows), y = num_rows)) +
    geom_bar(stat = "identity", fill = fill_color) +
    geom_text(aes(label = scales::comma(num_rows)), 
              vjust = -0.5, size = 3.5) +  # Add labels on top of the bars
    labs(title = title,
         x = "Base",
         y = "Number of Rows") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_y_continuous(labels = scales::comma)  # Add comma format to avoid scientific notation
}
```

```{r}
# Apply the function to the 1000m dataset
plot_base_counts(train_filtered_1000m_data, "Number of Rows per Base (1000m Data)", "lightblue")
```

```{r}
# Apply the function to the 2000m dataset
plot_base_counts(train_filtered_2000m_data, "Number of Rows per Base (2000m Data)", "lightgreen")
```

```{r}
# Function to plot timeline of Uber pickups per day for each base
plot_base_timeline <- function(data, title) {
  # Extract the date from the timestamp and group by base and date
  base_daily_counts <- data %>%
    mutate(date = as.Date(timestamp)) %>%
    group_by(base, date) %>%
    summarise(count_pickups_per_day = n()) %>%
    ungroup()

  # Plot the data
  ggplot(base_daily_counts, aes(x = date, y = count_pickups_per_day, color = base, group = base)) +
    geom_line(size = 1) +
    labs(title = title,
         x = "Date",
         y = "Number of Pickups",
         color = "Base") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```

```{r}
# Apply the function to your dataset
plot_base_timeline(train_filtered_1000m_data, "Uber Pickups per Day by Base (1000m Data)")
```

```{r}
# Apply the function to your dataset
plot_base_timeline(train_filtered_2000m_data, "Uber Pickups per Day by Base (2000m Data)")
```

```{r}
plot_base_usage_by_month <- function(data, data_name) {
  # Add a 'month' column to the data for grouping by month
  data <- data %>%
    mutate(month = floor_date(time_interval, "month"))
  
  # Aggregate the data to handle duplicate time_intervals, summing the number of pickups for each base at each time interval
  data <- data %>%
    group_by(time_interval, base) %>%
    summarise(number_of_pickups = sum(number_of_pickups), .groups = "drop")  # Added .groups = "drop" to avoid message

  # Split data by month
  monthly_data <- split(data, floor_date(data$time_interval, "month"))
  
  # Loop over each month and create a plot
  for (month_data in monthly_data) {
    # Get the month name for the title
    month_name <- format(unique(floor_date(month_data$time_interval, "month")), "%B %Y")
    
    # Create the plot
    p <- ggplot(month_data, aes(x = time_interval, y = number_of_pickups, color = base)) +
      geom_line(size = 1) +  # Line plot to show usage over time
      labs(
        title = paste("Base Usage Over Time -", month_name, "for:", data_name),
        x = "Time Interval",
        y = "Number of Pickups"
      ) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
    
    # Print the plot for the current month
    print(p)
  }
}

```

```{r}
plot_base_usage_by_month(train_filtered_1000m_data, "1000m data")
```

```{r}
plot_base_usage_by_month(train_filtered_2000m_data, "2000m data")
```

```{r}
# Function to create 6 bar plots for each base showing the sum of pickups for every day of the month
plot_daily_pickups_by_base <- function(data, data_name, color) {
  # Add a 'day' column to the data for grouping by day
  data <- data %>%
    mutate(day = floor_date(time_interval, "day"))
  
  # Group by base and day, and calculate the total number of pickups for each base on each day
  daily_pickups <- data %>%
    group_by(day, base) %>%
    summarise(number_of_pickups = sum(number_of_pickups), .groups = "drop")
  
  # Split the data by base
  base_data <- split(daily_pickups, daily_pickups$base)
  
  # Loop over each base and create a bar plot
  for (base_name in names(base_data)) {
    # Get the data for the current base
    base_df <- base_data[[base_name]]
    
    # Create the bar plot
    p <- ggplot(base_df, aes(x = day, y = number_of_pickups)) +
      geom_bar(stat = "identity", fill = color, color = "black") +  # Bar plot
      labs(
        title = paste("Daily Pickups for:", data_name, "- Base", base_name),
        x = "Day",
        y = "Number of Pickups"
      ) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
    
    # Print the plot for the current base
    print(p)
  }
}
```

```{r}
plot_daily_pickups_by_base(train_filtered_1000m_data, "1000m data", "lightblue")
```
```{r}
plot_daily_pickups_by_base(train_filtered_2000m_data, "2000m data", "lightgreen")
```

For both 1000m and 2000m data:
* Base B02764 and B02512 have a significantly lower number of Uber pickups compared to the other bases. This pattern is consistent in both datasets.

* Despite the lower overall pickup counts, there is Uber representative from each of the bases in each of the months and days. Each base, including B02764 and B02512, consistently records pickups on every moth and day within the date range. This indicates that all bases are operational throughout the dataset, even though the pickup activity differs in scale.

* Given the consistent activity from all bases, we conclude that all bases are relevant to our analysis. We will therefore retain all records from all bases to ensure the model captures the entire range of Uber activity across the city.

```{r}
train_filtered_1000m_data
```

```{r}
train_filtered_2000m_data
```

### Remove Unnecessary Features
In this step, we are removing unnecessary features from both the 1000m and 2000m datasets to streamline the data for modeling. Specifically, we are removing the timestamp, lat, lon, base, and dist columns, which are not needed for our predictive models. Our goal is to retain only the time_interval and number_of_pickups columns. Additionally, we ensure that each time interval appears only once (without repeats), as duplicates can distort predictions.

```{r}
# Function to remove unnecessary features and keep unique time intervals
clean_data <- function(data) {
  data %>%
    select(time_interval, number_of_pickups) %>%  # Keep only relevant columns
    distinct(time_interval, .keep_all = TRUE)  # Ensure no duplicate time intervals
}
```

```{r}
# Apply the function to the 1000m dataset
train_filtered_1000m_data <- clean_data(train_filtered_1000m_data)
train_filtered_1000m_data
```

```{r}
# Apply the function to the 2000m dataset
train_filtered_2000m_data <- clean_data(train_filtered_2000m_data)
head(train_filtered_2000m_data)
```

Lets Make sure there are no duplicated time interval:
```{r}
# Function to get rows with duplicate time_interval values
get_duplicate_time_intervals <- function(data, time_interval) {
  
  # Identify duplicate time_intervals
  duplicate_intervals <- data[[time_interval]][duplicated(data[[time_interval]])]
  
  # Return only the rows that have duplicate time_intervals
  duplicate_rows <- data[data[[time_interval]] %in% duplicate_intervals, ]
  
  cat("There are: ", length(duplicate_intervals), "duplicate time intervals.\n")
  
  return(duplicate_rows)
}

```

```{r}
duplicate_rows_1000 <- get_duplicate_time_intervals(train_filtered_1000m_data, "time_interval")
duplicate_rows_1000 
```

```{r}
duplicate_rows_2000 <- get_duplicate_time_intervals(train_filtered_2000m_data, "time_interval")
duplicate_rows_2000 
```

```{r}
duplicate_rows_test <- get_duplicate_time_intervals(test_fixed_data, "time_interval")
duplicate_rows_test
```

### Dealing With Missing Time Intervals
Now, we would like to see if there are missing time intervals:
```{r}
find_missing_time_intervals <- function(data) {
  
  # Define the start and end time for the sequence
  start_time <- as.POSIXct("2014-04-01 17:00:00", tz = "UTC")
  end_time <- as.POSIXct("2014-09-09 23:45:00", tz = "UTC")
  
  # Generate the full sequence of expected 15-minute intervals
  expected_time_sequence <- seq(from = start_time, to = end_time, by = "15 min")
  
  # Filter the time sequence for intervals between 17:00 and 00:00 only
  expected_time_sequence <- expected_time_sequence[
    (format(expected_time_sequence, "%H") >= "17" & format(expected_time_sequence, "%H") <= "23") |
    (format(expected_time_sequence, "%H") == "00" & format(expected_time_sequence, "%M") == "00" & format(expected_time_sequence, "%Y-%m-%d") != "2014-04-01")
  ]
  
  # Extract unique time intervals from the actual data
  actual_intervals <- data %>%
    mutate(Unique_Interval = format(!!sym("time_interval"), "%Y-%m-%d %H:%M:%S")) %>%
    distinct(Unique_Interval)
  
  # Find the missing intervals
  missing_intervals <- setdiff(format(expected_time_sequence, "%Y-%m-%d %H:%M:%S"), actual_intervals$Unique_Interval)
  
  # Return the missing intervals in POSIXct format
  if (length(missing_intervals) > 0) {
    print("Missing date-time intervals:")
    return(as.POSIXct(missing_intervals, format = "%Y-%m-%d %H:%M:%S", tz = "UTC"))
  } else {
    print("No missing date-time intervals.")
    return(NULL)
  }
}
```

```{r}
missing_intervals_1000 <- find_missing_time_intervals(train_filtered_1000m_data)
missing_intervals_1000
```

```{r}
missing_intervals_2000 <-find_missing_time_intervals(train_filtered_2000m_data)
missing_intervals_2000
```

```{r}
range(pickup_counts_1000m$time_interval)
```

```{r}
range(pickup_counts_2000m$time_interval)
```

```{r}
train_filtered_1000m_data
```

Lets add the missing time intervals into the dataset:
```{r}
add_missing_intervals <- function(data, missing_intervals) {
  
  # Create a data frame for the missing intervals with 0 pickups
  missing_data <- data.frame(
    time_interval = missing_intervals,    # Add the missing intervals
    number_of_pickups = 0                 # Assign 0 pickups for each missing interval
  )
  
  # Bind the missing data with the original data
  updated_data <- rbind(data, missing_data)
  
  # Sort the data by time_interval to keep it in the correct order
  updated_data <- updated_data[order(updated_data$time_interval), ]
  
  return(updated_data)
}
```

```{r}
train_filtered_1000m_data <- add_missing_intervals(train_filtered_1000m_data, missing_intervals_1000)
train_filtered_2000m_data <- add_missing_intervals(train_filtered_2000m_data, missing_intervals_2000)
```

```{r}
train_filtered_1000m_data
```

Lets see the rows added and that number_of_pickups is 0:
```{r}
train_filtered_1000m_data[train_filtered_1000m_data$number_of_pickups == 0, ]
```

```{r}
train_filtered_2000m_data
```

```{r}
train_filtered_2000m_data[train_filtered_2000m_data$number_of_pickups == 0, ]
```


## Part B: Exploratory Data Analysis (EDA)
### 1. Data Preparation
#### Date base data: Adding Columns
We added in the Data Rearrangement part:
1. number_of_pickups: this is how many pickups were is this specific 15-min interval (the label).

2. dist (for the 1000m data): this is the distance from each pickup to the Empire State Building

We will also add:
1.is_weekend: Extract whether the pickup occurred on a weekday or weekend. This is important because pickup patterns often differ on weekdays versus weekends.

2. is_night: binary feature to identify if the time interval is in the evening or night to capture.

##### is_weekend
Add The is_weekend Feature:
```{r adding_is_weekend}
# Function to add 'is_weekend' feature using time_interval
add_is_weekend <- function(data) {
  data %>%
    mutate(
      is_weekend = ifelse(wday(time_interval, label = TRUE) %in% c("Sat", "Sun"), 1, 0)  # Check if it's a weekend (Saturday or Sunday)
    )
}
```

```{r}
# Apply the function to the 1000m dataset
train_filtered_1000m_data <- add_is_weekend(train_filtered_1000m_data)
train_filtered_1000m_data
```

```{r}
# Apply the function to the 2000m dataset
train_filtered_2000m_data <- add_is_weekend(train_filtered_2000m_data)
train_filtered_2000m_data
```

```{r}
# Apply the function to the test dataset
test_fixed_data <- add_is_weekend(test_fixed_data)
test_fixed_data
```

##### is_night
Add The time_of_day Feature:

Note: In this case, since our data contains times only between 17:00 and 00:00, we will adapt the time_of_day feature to reflect only the relevant periods: Evening (17:00-20:30) and Night (20:31-00:00). There will be no Afternoon or Morning periods as the data doesn't cover these hours.

Function to add 'is_night' as a binary feature (0 for Evening, 1 for Night). This feature assigns a binary value based on the time of day. 
In our case, Evening (17:00 - 20:30) is represented by 0, and Night (20:31 - 00:00) is represented by 1. 

```{r}
# Function to add 'is_night' as a binary feature (0 for Evening, 1 for Night)
add_is_night  <- function(data) {
  data %>%
    mutate(
      is_night = case_when(
        # Evening: 17:00 - 20:30
        hour(time_interval) == 17 ~ 0,                              
        hour(time_interval) > 17 & hour(time_interval) < 20 ~ 0,        
        hour(time_interval) == 20 & minute(time_interval) <= 30 ~ 0, 
        
        # Night: 20:31 - 00:00
        (hour(time_interval) == 20 & minute(time_interval) > 30) |      
        (hour(time_interval) > 20 & hour(time_interval) <= 23) ~ 1,
        hour(time_interval) == 0 ~ 1
      )
    )
}
```

```{r}
# Apply the function to the 1000m dataset
train_filtered_1000m_data <- add_is_night(train_filtered_1000m_data)
train_filtered_1000m_data
```

```{r}
# Apply the function to the 2000m dataset
train_filtered_2000m_data <- add_is_night(train_filtered_2000m_data)
train_filtered_2000m_data
```

```{r}
# Apply the function to the test dataset
test_fixed_data <- add_is_night(test_fixed_data)
test_fixed_data
```

#### Data from the Web: External Data Integration

##### Weather
Here is a CSV of the history weather bulk for Empire State Building (40.75,-73.99) from January 01, 2014 to December 31, 2014.
* The parameters of the data explained here: https://openweathermap.org/history-bulk

```{r}
# Load the CSV file
weather_data <- read_csv("empire-state-weather-2014.csv")
weather_data
```

```{r}
# Convert the 'dt_iso' column to POSIXct format and create the new 'timestamp' column
weather_data <- weather_data %>%
  mutate(timestamp = as.POSIXct(dt_iso, format = "%Y-%m-%d %H:%M:%S", tz = "UTC"))

# Create 'is_raining_last_hour' and 'is_snowing_last_hour' features
weather_data <- weather_data %>%
  mutate(is_raining_last_hour = ifelse(!is.na(rain_1h) & rain_1h > 0, 1, 0),
         is_snowing_last_hour = ifelse(!is.na(snow_1h) & snow_1h > 0, 1, 0))

weather_data
```

Code to Check Minutes and Seconds in Timestamps:
```{r}
# Filter rows where the minute or second is not 0
non_zero_minute_or_second <- weather_data %>%
  filter(minute(timestamp) != 0 | second(timestamp) != 0)

# Check if there are any rows with non-zero minute or second values
if (nrow(non_zero_minute_or_second) > 0) {
  print("Rows with non-zero minutes or seconds found:")
  print(non_zero_minute_or_second)
} else {
  print("All timestamps have 0 for both minutes and seconds.")
}
```

We have confirmed that the weather data is recorded at exact hourly intervals, meaning the data is consistent without variations in the minutes or seconds. To merge this data with our Uber pickup data, we need to ensure our timestamps align correctly. Since the weather information is relevant for each hour as recorded, we will always round the Uber pickup timestamps down to the current hour to maintain accuracy and ensure a seamless merge.

Additionally, to reflect the hourly nature of the data, we will rename the timestamp column to hourly_time_interval to indicate that it represents the start of each hour.

```{r}
# Rename the 'timestamp' column to 'hourly_time_interval'
weather_data <- weather_data %>%
  rename(hourly_time_interval = timestamp)

# Check the first few rows to verify the change
head(weather_data)
```

```{r}
range(weather_data$hourly_time_interval)
```

```{r}
range(train_filtered_1000m_data$time_interval)
```

```{r}
range(train_filtered_2000m_data$time_interval)
```

```{r}
range(test_fixed_data$time_interval)
```

The weather_data dataset spans from 2014-01-01 00:00:00 to 2014-12-31 23:00:00, covering the entire time range present in both the training and test data.

We combine weather data (temperature, humidity, wind speed, and snow indication) with the Uber pickup data (1000m and 2000m datasets) using the hourly_time_interval as the key. We round all Uber pickup timestamps (i.e., the time_interval) down to the current hour to match the weather data, which is recorded at one-hour intervals. This approach ensures that the weather conditions are consistently matched with the Uber pickup data at the appropriate hourly time.

We are using a function called rounded_timestamp that adds a new feature to the Uber pickup datasets. This feature rounds the time_interval according to the following rules: if the time_interval has minutes of 00 or 15, it is rounded down to the current hour; if it has minutes of 30 or 45, it is rounded up to the next hour. This ensures that the Uber pickup data aligns with the hourly_time_interval in the weather data, which is recorded at one-hour intervals.
```{r}
# Function to round the time_interval and ensure it's in POSIXct format
rounded_timestamp <- function(df) {
  df %>%
    mutate(
      rounded_timestamp = case_when(
        minute(time_interval) %in% c(30, 45) ~ as.POSIXct(ceiling_date(time_interval, "hour"), tz = "UTC"),
        minute(time_interval) %in% c(0, 15) ~ as.POSIXct(floor_date(time_interval, "hour"), tz = "UTC")
      )
    )
}
```


```{r}
# Round time_interval for 1000m data
train_filtered_1000m_data <- rounded_timestamp(train_filtered_1000m_data)
train_filtered_1000m_data
```

```{r}
# Round time_interval for 2000m data
train_filtered_2000m_data <- rounded_timestamp(train_filtered_2000m_data)
train_filtered_2000m_data
```

```{r}
# Round time_interval for test data
test_fixed_data <- rounded_timestamp(test_fixed_data)
test_fixed_data
```

```{r}
weather_data
```

We will make sure there are no duplicates in the data:
```{r}
get_duplicate_time_intervals(weather_data, "hourly_time_interval")
```

Lets drop the duplicates so that the merge with our data wont get duplicates as well:
```{r}
# Function to drop duplicates based on the time_interval
drop_duplicate_time_intervals <- function(data, time_interval) {
  
  # Remove duplicates based on the specified time_interval column, keeping the first occurrence
  data_deduplicated <- data[!duplicated(data[[time_interval]]), ]
  
  cat("Duplicates removed. Remaining rows: ", nrow(data_deduplicated), "\n")
  
  return(data_deduplicated)
}
```

```{r}
weather_data <- drop_duplicate_time_intervals(weather_data, "hourly_time_interval")
```

```{r}
get_duplicate_time_intervals(weather_data, "hourly_time_interval")
```

```{r}
weather_data
```

Select the Required Columns from the Weather Data:
```{r}
# Select only the required columns from the weather data
weather_data_selected <- weather_data %>%
  select(hourly_time_interval, temp, humidity, wind_speed, feels_like, clouds_all, is_raining_last_hour, is_snowing_last_hour)
```

```{r}
weather_data_selected
```

The features we added:
Snow (is_snowing_last_hour): Snow volume for the last hour, mm (in liquid state).

Rain (is_raining_last_hour): Rain volume for the last hour, mm.

temp: Temperature.

feels_like:This temperature parameter accounts for the human perception of weather.

Wind speed: Wind speed. Units â€“ default: meter/sec.

clouds_all: Cloudiness, %.

humidity: Humidity, %.

```{r}
train_filtered_1000m_data
```

```{r}
duplicate_rows_1000 <- get_duplicate_time_intervals(train_filtered_1000m_data, "time_interval") 
duplicate_rows_1000
```

Merging Weather Data with 1000m & 2000m & test Data Based on rounded_timestamp and hourly_time_interval:
```{r}
# Merge the weather data with the 1000m dataset using 'rounded_timestamp' and 'hourly_time_interval'
train_filtered_1000m_data <- train_filtered_1000m_data %>%
  left_join(weather_data_selected, by = c("rounded_timestamp" = "hourly_time_interval"))

train_filtered_1000m_data
```

```{r}
get_duplicate_time_intervals(train_filtered_1000m_data, "time_interval")
```


```{r}
# Merge the weather data with the 2000m dataset using 'rounded_timestamp' and 'hourly_time_interval'
train_filtered_2000m_data <- train_filtered_2000m_data %>%
  left_join(weather_data_selected, by = c("rounded_timestamp" = "hourly_time_interval"))

# Check the first few rows of the updated 2000m dataset
head(train_filtered_2000m_data)
```

```{r}
# Merge the weather data with the test dataset using 'rounded_timestamp' and 'hourly_time_interval'
test_fixed_data <- test_fixed_data %>%
  left_join(weather_data_selected, by = c("rounded_timestamp" = "hourly_time_interval"))

# Check the first few rows of the updated 2000m dataset
head(test_fixed_data)
```

Notes:
* Merging Based on hourly_time_interval: The weather data uses hourly_time_interval as its timestamp, while the Uber data uses rounded_timestamp, which we created to match the hourly granularity of the weather data.

* Preserving All Rows from Uber Data: We use a left_join, which ensures that all rows from train_filtered_1000m_data and train_filtered_2000m_data are preserved. If there is no matching weather data for a given rounded_timestamp, the weather columns will contain NA.

* No Additional Rows from Weather Data: The weather data is not expanded to include extra rows in the Uber pickup datasets. Only the relevant weather columns are added to the existing rows in the Uber data.

##### Crashes
Here we will add data about motor vehicle collisions in New York City: https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95/about_data. By incorporating data from the Motor Vehicle Collisions - Crashes dataset, we aim to capture how traffic accidents may impact Uber demand. Traffic congestion and accidents can lead to delays or reduced availability of Uber vehicles in certain areas, affecting both the supply of drivers and the demand for rides.

After pre-processing we did on the raw data we fot two CSV files (crashes_sums_1000m.csv and crashes_sums_2000m.csv) that contain crash data within and above a 1000-meter and 2000-meter radius of the Empire State Building, respectively. The columns include:
* crash_time_15min: The timestamp for each 15-minute interval.
* number_of_persons_injured: The number of people injured in crashes within that time interval.
* number_of_persons_killed: The number of people killed in crashes within that time interval.
* number_of_crashes: The total number of crashes occurring within that time interval.

How This Data Can Be Helpful for Predictions:
1. Traffic Impact on Uber Demand:
High numbers of crashes, especially those causing injuries or fatalities, could lead to significant traffic disruptions. These disruptions may reduce the number of available Uber vehicles, or create increased demand as people opt for ride-hailing services over other transportation options.
By correlating the crash data with Uber pickup data, you may notice patterns where crash events (even within specific time intervals) affect demand. This correlation could provide valuable insight for predicting Uber demand, particularly during peak traffic hours or in highly trafficked areas.

2. Safety Concerns:
Time periods or areas with higher crash rates might temporarily reduce Uber ridership as people become more cautious about traveling, potentially affecting demand patterns.
By including this crash data, you can capture these safety concerns and measure their influence on demand.

3. Modeling the Impact:
By incorporating the number_of_persons_injured, number_of_persons_killed, and number_of_crashes features from this crash data, you can directly account for the effect of accidents on Uber demand.
Crashes can be treated as external factors contributing to demand variability. In your model, you can test how these variables affect demand forecasting, which could provide more accurate predictions, especially in periods with high crash activity.
```{r load_data}
crashes_sums_1000m <- read.csv("crashes_sums_1000m.csv")
crashes_sums_2000Am <- read.csv("crashes_sums_2000Am.csv")
```

```{r}
crashes_sums_1000m
```

```{r}
crashes_sums_2000Am
```

Duplicate time check:
```{r}
get_duplicate_time_intervals(train_filtered_1000m_data, "crash_time_15min") 
```
```{r}
get_duplicate_time_intervals(train_filtered_2000m_data, "crash_time_15min") 
```

fix crash_time_15min format:
(We will use a general function that can be useful in the future)
```{r}
convert_time_to_datetime <- function(data, time_column) {
  data %>%
    mutate(!!time_column := as.POSIXct(!!sym(time_column), format = "%Y-%m-%d %H:%M:%S", tz = "UTC"))
}
```

```{r}
# Usage for 1000m crash data
crashes_sums_1000m <- convert_time_to_datetime(crashes_sums_1000m, "crash_time_15min")
crashes_sums_1000m
```

```{r}
# Usage for 2000m crash data
crashes_sums_2000Am <- convert_time_to_datetime(crashes_sums_2000Am, "crash_time_15min")
crashes_sums_2000Am
```

Checking time range:
```{r}
range(crashes_sums_1000m$crash_time_15min)
```

```{r}
range(crashes_sums_2000Am$crash_time_15min)
```

Here we will merge the crash data features with train_filtered_1000m_data & train_filtered_2000m_data & test_fixed_data using left_join where crash_time_15min = time_interval:
```{r}
# Function to merge crash data with train or test data
merge_crash_data <- function(data, crash_data) {
  data %>%
    left_join(crash_data, by = c("time_interval" = "crash_time_15min"))
}
```

```{r}
# Merge crash data with the 1000m Uber dataset
train_filtered_1000m_data <- merge_crash_data(train_filtered_1000m_data, crashes_sums_1000m)
train_filtered_1000m_data
```

```{r}
# Merge crash data with the 2000m Uber dataset
train_filtered_2000m_data <- merge_crash_data(train_filtered_2000m_data, crashes_sums_2000Am)
train_filtered_2000m_data
```

```{r}
# Merge crash data with the test Uber dataset
test_fixed_data <- merge_crash_data(test_fixed_data, crashes_sums_1000m)
test_fixed_data
```

##### Events
Here we will add data about NYC Permitted Events (https://data.cityofnewyork.us/City-Government/NYC-Permitted-Event-Information-Historical/bkfu-528j/about_data), which include all types of public events that have received a permit from the city. These events range from parades, block parties, concerts, and street fairs to film shoots and other public gatherings that occur throughout the city. This data, after some pre-processing we did includes the number of events that occurred in specific time intervals, which could influence Uber ride demand, as larger events may cause an increase in ride requests due to the influx of people.

The event data includes the following columns:
* time_interval: The 15-minute interval when the events occurred, which allows us to track event activity over time.
* event_count: The number of events happening during that specific 15-minute interval. Higher event counts may indicate more activity and potentially greater Uber demand.

Types of events covered in this data include:
* Parades: City-wide or borough-specific parades that often require road closures and generate significant foot traffic.
* Street Fairs: Neighborhood fairs that can cause localized congestion and boost demand for transportation services.
* Film Shoots: Movie or TV productions that can lead to street closures or restricted access.
* Concerts and Festivals: Major events that attract large crowds and could cause spikes in Uber demand before and after the event.
* Block Parties: Local gatherings that block off residential streets, possibly leading to increased Uber rides in and out of the area.

This information can help our model predictions by accounting for:
* Increased Demand: Major events often lead to spikes in transportation demand, particularly at the start and end times of events.
* Traffic Disruptions: Some events involve street closures or restrictions, which may cause delays or alter Uber routes, potentially reducing the number of available rides in the area.
* Predicting Demand Patterns: By incorporating event data into our model, we can identify patterns where Uber demand surges during large gatherings or city-wide celebrations, which helps improve the accuracy of our pickup predictions around the Empire State Building and surrounding areas.

```{r load_data}
nyc_event_counts <- read.csv("nyc_event_counts.csv")
nyc_event_counts
```

fix the time interval format:
```{r}
nyc_event_counts <- convert_time_to_datetime(nyc_event_counts, "time_interval")
nyc_event_counts
```

Duplicate time check:
```{r}
get_duplicate_time_intervals(nyc_event_counts, "time_interval") 
```

Checking time range:
```{r}
range(nyc_event_counts$time_interval)
```

Here we will merge the nyc_event_counts dataset with train_filtered_1000m_data, train_filtered_2000m_data, and test_fixed_data using a left_join on the time_interval column:
```{r}
# Function to merge event data with train or test data
merge_event_data <- function(data, event_data) {
  data %>%
    left_join(event_data, by = "time_interval")
}
```

```{r}
# Merge event data with train_filtered_1000m_data
train_filtered_1000m_data <- merge_event_data(train_filtered_1000m_data, nyc_event_counts)
train_filtered_1000m_data
```

```{r}
# Merge event data with train_filtered_2000m_data
train_filtered_2000m_data <- merge_event_data(train_filtered_2000m_data, nyc_event_counts)
train_filtered_2000m_data
```

```{r}
# Merge event data with test_fixed_data
test_fixed_data <- merge_event_data(test_fixed_data, nyc_event_counts)
test_fixed_data
```

##### Yellow Taxis
We incorporated data from the NYC Yellow Taxi Trip Records, which provides detailed information on taxi trips across New York City. This data is publicly available and can be accessed from the NYC Taxi & Limousine Commission (TLC): https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page.
The raw data includes details such as pickup and drop-off times, pickup and drop-off locations, fare amounts, distance traveled, and more.

To align the Yellow Taxi data with our Uber pickup predictions, we performed the following preprocessing steps:

* 15-Minute Time Intervals: We grouped the pickup times into 15-minute intervals to standardize the data. This allows us to compare Uber and Yellow Taxi demand on the same time scale, making it easier to identify patterns and trends in transportation demand.
* Pickup Count: For each 15-minute interval, we counted the number of yellow taxi pickups, providing a pickup_count feature for each time block. This pickup count represents the total number of taxi rides that started within that 15-minute period.
* Geographical Filtering: We split the data into two separate datasets:
1. Within 1000 meters: Data representing taxi pickups that occurred within a 1000-meter radius from the Empire State Building.
2. Above 2000 meters: Data representing taxi pickups that occurred more than 2000 meters away from the Empire State Building.

Merging yellow taxi data with Uber pickup data can improve Uber ride predictions in several ways:
* Substitute Transportation: When yellow taxi pickups increase, Uber demand may decrease as passengers choose taxis. This helps model shifts between transportation modes for more accurate predictions.
* Complementary Demand: Both Uber and taxis may see increased demand during busy periods (events, weather, holidays). Combining these datasets captures overall transportation trends more effectively.
* Traffic & Supply: High taxi activity can signal traffic or a larger supply of vehicles, affecting Uber availability and demand.
```{r load_data}
yellow_taxis_1000m <- read.csv("yellow_taxis_pickup_counts_1000m.csv")
yellow_taxis_2000Am <- read.csv("yellow_taxis_pickup_counts_2000Am.csv")
```

```{r}
yellow_taxis_1000m
```

```{r}
yellow_taxis_2000Am
```

fix crash_time_15min format:
```{r}
# Usage for 1000m crash data
yellow_taxis_1000m <- convert_time_to_datetime(yellow_taxis_1000m, "pickup_time_15min")
yellow_taxis_1000m
```

```{r}
# Usage for 2000m crash data
yellow_taxis_2000Am <- convert_time_to_datetime(yellow_taxis_2000Am, "pickup_time_15min")
yellow_taxis_2000Am
```

Duplicate time check:
```{r}
get_duplicate_time_intervals(yellow_taxis_1000m, "pickup_time_15min") 
```

```{r}
get_duplicate_time_intervals(yellow_taxis_2000Am, "pickup_time_15min") 
```

Checking time range:
```{r}
range(yellow_taxis_1000m$pickup_time_15min)
```

```{r}
range(yellow_taxis_2000Am$pickup_time_15min)
```
For our convenience, we will rename the pickup_count column to taxis_pickup_count:
```{r}
# Function to rename 'pickup_count' to 'taxis_pickup_count'
rename_pickup_count <- function(data) {
  data %>%
    rename(taxis_pickup_count = pickup_count)
}
```

```{r}
# Apply the function to the datasets
yellow_taxis_1000m <- rename_pickup_count(yellow_taxis_1000m)
yellow_taxis_2000Am <- rename_pickup_count(yellow_taxis_2000Am)
```

```{r}
yellow_taxis_1000m
```

```{r}
yellow_taxis_2000Am
```

Here we will merge the yellow taxis data pickup count with train_filtered_1000m_data & train_filtered_2000m_data & test_fixed_data using left_join where pickup_time_15min = time_interval:
```{r}
# Function to merge yellow taxis data with train or test data
merge_taxis_data <- function(data, yellow_taxis_data) {
  data %>%
    left_join(yellow_taxis_data, by = c("time_interval" = "pickup_time_15min"))
}
```

```{r}
# Merge yellow_taxis data with the 1000m Uber dataset
train_filtered_1000m_data <- merge_taxis_data(train_filtered_1000m_data, yellow_taxis_1000m)
train_filtered_1000m_data
```

```{r}
# Merge yellow_taxis data with the 2000m Uber dataset
train_filtered_2000m_data <- merge_taxis_data(train_filtered_2000m_data, yellow_taxis_2000Am)
train_filtered_2000m_data
```

```{r}
# Merge yellow_taxis data with the test Uber dataset
test_fixed_data <- merge_taxis_data(test_fixed_data, yellow_taxis_1000m)
test_fixed_data
```

##### Arrests
The dataset contains information about arrests made by the NYPD (https://data.cityofnewyork.us/Public-Safety/NYPD-Arrests-Data-Historic-/8h9b-rp9u/about_data), categorized by date, felony counts, misdemeanor counts, and total arrests. After preprocessing, we split the data into two categories: arrests within a 1000-meter radius of the Empire State Building and those occurring beyond 2000 meters. 

Explanation of Features:
* arrest_date: The specific date on which the arrests were made.
* felony_count: The number of felony arrests made on that day.
* misdemeanor_count: The number of misdemeanor arrests made on that day.
* total_arrests: The total number of arrests (felony + misdemeanor) made on that day.

How Merging This Data Can Help Final Predictions:
* Crime and Public Perception: High arrest counts, especially for felonies, may deter people from certain areas, lowering Uber demand. Fewer arrests could suggest a safer environment, increasing ridership.
* Traffic Disruptions: Large-scale arrests can lead to roadblocks, affecting transportation. This helps us model how such events impact Uber availability and wait times.
* Safety and Demand Correlation: Arrest data helps us understand how safety concerns affect Uber demand, improving prediction accuracy during high-crime periods.
* Geographical Impact: Splitting the data into 1000-meter and 2000-meter categories allows us to better understand how crime near the Empire State Building influences Uber demand.
```{r load_data}
arrests_1000m <- read.csv("arrests_F_and_M_1000m.csv")
arrests_2000Am <- read.csv("arrests_F_and_M_2000Am.csv")
```

To avoid reaching a state of complete multicollinearity, we will remove both felony_count and misdemeanor_count, keeping only total_arrests for our predictions. This will ensure that the model does not suffer from redundancy due to the perfect correlation between total_arrests, felony_count, and misdemeanor_count.
```{r}
# Function to remove 'felony_count' and 'misdemeanor_count' from the dataset
clean_arrest_data <- function(data) {
  # Remove 'felony_count' and 'misdemeanor_count' columns
  data_cleaned <- data %>%
    select(-felony_count, -misdemeanor_count)
  
  # Return the cleaned dataset
  return(data_cleaned)
}
```

```{r}
arrests_1000m <- clean_arrest_data(arrests_1000m)
arrests_2000Am <- clean_arrest_data(arrests_2000Am)   
```

```{r}
arrests_1000m
```

```{r}
arrests_2000Am
```

Duplicate time check:
```{r}
get_duplicate_time_intervals(arrests_1000m, "arrest_date") 
```

```{r}
get_duplicate_time_intervals(arrests_2000Am, "arrest_date") 
```

Transform Daily Arrest Data to 15-Minute Intervals:
```{r}

```

```{r}
# Function to replicate daily arrests data across all 15-minute intervals in each day
replicate_arrests_to_intervals <- function(arrests_data) {
  # Convert the arrest_date to Date format
  arrests_data <- arrests_data %>%
    mutate(arrest_date = as.Date(arrest_date, format = "%Y-%m-%d"))
  
  # Create a sequence of 15-minute intervals for each day and replicate arrests count for each interval
  arrests_data_15min <- arrests_data %>%
    rowwise() %>%
    mutate(
      time_intervals = list(seq(
        from = as.POSIXct(paste(arrest_date, "00:00:00"), tz = "UTC"),
        to = as.POSIXct(paste(arrest_date, "23:45:00"), tz = "UTC"),
        by = "15 min"
      ))
    ) %>%
    unnest(cols = c(time_intervals)) %>%
    mutate(total_arrests = total_arrests) %>%
    select(time_interval = time_intervals, total_arrests)
  
  return(arrests_data_15min)
}
```

```{r}
# Apply the function to the datasets
arrests_1000m <- replicate_arrests_to_intervals(arrests_1000m)
arrests_2000Am <- replicate_arrests_to_intervals(arrests_2000Am)
```

```{r}
arrests_1000m
```

```{r}
arrests_2000Am
```

Checking time range:
```{r}
range(arrests_1000m$time_interval)
```

```{r}
range(arrests_2000Am$time_interval)
```
Here we will merge the arrests data pickup with train_filtered_1000m_data & train_filtered_2000m_data & test_fixed_data using left_join by time_interval:
```{r}
# Function to merge yellow taxis data with train or test data
merge_arrests_data <- function(data, arrests_data) {
  data %>%
    left_join(arrests_data, by = "time_interval")
}
```

```{r}
# Merge arrests data with the 1000m Uber dataset
train_filtered_1000m_data <- merge_arrests_data(train_filtered_1000m_data, arrests_1000m)
train_filtered_1000m_data
```

```{r}
# Merge arrests data with the 2000m Uber dataset
train_filtered_2000m_data <- merge_arrests_data(train_filtered_2000m_data, arrests_2000Am)
train_filtered_2000m_data
```

```{r}
# Merge arrests data with the test Uber dataset
test_fixed_data <- merge_arrests_data(test_fixed_data, arrests_1000m)
test_fixed_data
```

##### Holidays
We incorporated a table of all U.S. federal holidays with their dates in 2014: https://www.kaggle.com/datasets/jeremygerdes/us-federal-pay-and-leave-holidays-2004-to-2100-csv?select=400_Years_of_Generated_Dates_and_Holidays.csv. 

How Merging Holiday Data Can Help Final Predictions:
* Impact on Uber Demand: Federal holidays often see a shift in transportation demand. Holidays typically lead to changes in commuting patterns, as fewer people commute to work and more travel for leisure, which could increase Uber demand.
* Event-Driven Peaks: Special events, gatherings, and increased social activities on holidays can create spikes in Uber ride demand, particularly near popular locations like the Empire State Building.
* Predicting Holiday Traffic: Including holidays allows us to predict periods of increased or decreased Uber demand based on public events or reduced commuting activity. This helps refine our predictions by accounting for known holiday effects on transportation patterns.

```{r load_data}
holidays <- read.csv("US_2014_federal_holidays.csv")
holidays
```

We need to filter the holidays that occur between April 1, 2014, and September 30, 2014:
```{r}
# Convert the 'Date' column to Date format
holidays <- holidays %>%
  mutate(Date = as.Date(Date, format = "%Y-%m-%d"))

# Filter holidays between 1st April 2014 and 30th September 2014
holidays <- holidays %>%
  filter(Date >= as.Date("2014-04-01") & Date <= as.Date("2014-09-30"))

# View the filtered holidays
holidays
```

The only holidays in the range of our data are: Memorial Day, Independence Day, Labor Day.
Here's the function that adds new holiday features (Memorial Day, Independence Day, and Labor Day) to each of your datasets based on whether the time_interval falls on a specific holiday:
```{r}
# Function to add holiday features to a dataset
add_holiday_features <- function(data) {
  data %>%
    mutate(
      # Check if each time_interval matches the specific holiday and create a binary column for each
      memorial_day = ifelse(format(as.Date(time_interval, tz = "UTC"), "%Y-%m-%d") == "2014-05-26", 1, 0),
      independence_day = ifelse(format(as.Date(time_interval, tz = "UTC"), "%Y-%m-%d") == "2014-07-04", 1, 0),
      labor_day = ifelse(format(as.Date(time_interval, tz = "UTC"), "%Y-%m-%d") == "2014-09-01", 1, 0)
    )
}
```

```{r}
# Apply the function to each dataset
train_filtered_1000m_data <- add_holiday_features(train_filtered_1000m_data)
train_filtered_2000m_data <- add_holiday_features(train_filtered_2000m_data)
test_fixed_data <- add_holiday_features(test_fixed_data)
```

```{r}
train_filtered_1000m_data
```

```{r}
train_filtered_2000m_data
```

```{r}
test_fixed_data
```

If all the features-Memorial_Day, Independence_Day, and Labor_Day-are set to 0, it indicates that the date is a regular day (not a holiday).

Lets remove the rounded_timestamp because its not necessary:
```{r}
# Function to remove the 'rounded_timestamp' column from the dataset
remove_rounded_timestamp <- function(data) {
  data %>%
    select(-rounded_timestamp)
}
```

```{r}
train_filtered_1000m_data <- remove_rounded_timestamp(train_filtered_1000m_data)
```

```{r}
train_filtered_2000m_data <- remove_rounded_timestamp(train_filtered_2000m_data)
```

```{r}
test_fixed_data <- remove_rounded_timestamp(test_fixed_data)
```


Lets check NAs after adding new features:
```{r}
# For 1000m data
nas_1000m <- count_nas_per_column(train_filtered_1000m_data)
print("NA counts for 1000m data:")
print(nas_1000m)
```

```{r}
# For 2000m data
nas_2000m <- count_nas_per_column(train_filtered_2000m_data)
print("NA counts for 2000m data:")
print(nas_2000m)
```
No NAs!

```{r}
# For test data
nas_test <- count_nas_per_column(test_fixed_data)
print("NA counts for test data:")
print(nas_test)
```
NAs for only number_of_pickups - this is what we need to predict          

```{r}
# Assuming your table (data frame) is named 'my_table'
# write.csv(train_filtered_1000m_data, "train_filtered_1000m_data.csv")
# write.csv(train_filtered_2000m_data, "train_filtered_2000m_data.csv")
```

### 2. Exploratory Analysis
1. Actions Taken on the Data (Data Cleaning and Transformation):
Start by documenting what actions have been performed on the data:

Filtered Data: Data was filtered based on distance from the Empire State Building:
1. train_raw_data which is now called train_filtered_1000m_data table which has only data about pickups which was occurred up to 1000m radius from the Empire State Building.
2. The train_filtered_2000m_data includes pickups from over 2000 meters away from the Empire State Building. 

New Variables: The dataset includes the columns: time_interval, number_of_pickups,	is_weekend, is_night, temp, humidity, wind_speed, feels_like, clouds_all, is_raining_last_hour, is_snowing_last_hour, number_of_persons_injured, number_of_persons_killed, number_of_crashes, event_count, taxis_pickup_count, total_arrests, memorial_day, independence_day, labor_day  

We added a lot of features, we probably won't need all of them, we'll start with dry logic and then move on to tests and observations.

```{r}
numerical_features <- c(
  "number_of_pickups", "temp", "humidity", "wind_speed", "feels_like", 
  "clouds_all", "number_of_persons_injured", "number_of_persons_killed", 
  "number_of_crashes", "event_count", "taxis_pickup_count", "total_arrests"
)

numerical_features
```
```{r}
categorical_features <- c("is_weekend", "is_night", "is_raining_last_hour", "is_snowing_last_hour", 
                         "memorial_day", "independence_day", "labor_day")
categorical_features
```

#### Looking At The Features 
Firstly, we will use correlation analysis to:
1. determine which features are less likely to explain or predict the number of pickups (features that have low correlation with number_of_pickups)
2. Identify potential multicollinearity between features. Multicollinearity occurs when two or more features are highly correlated with each other. In cases of high correlation between features, one of the features can be removed to prevent redundancy and to ensure that the model does not become unstable. Reducing multicollinearity improves the interpretability of the model and ensures that the model can better distinguish the effects of different variables.


```{r}
# Function for correlation analysis and visualization
create_corr_matrix <- function(data, dataset_name, numerical_features) {
  # Select only the numerical columns specified in the numerical_features list
  numeric_data <- data %>%
    select(all_of(numerical_features))  # Select specified numerical columns

  # Calculate correlation matrix using 'pairwise.complete.obs' to handle missing values
  cor_matrix <- cor(numeric_data, use = "pairwise.complete.obs", method = "pearson")
  
  # Replace NaNs or any Inf values with NA to avoid issues
  cor_matrix[is.na(cor_matrix)] <- NA
  
  # Create a correlation plot with adjusted label sizes
  col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
  
  # Plot the full correlation matrix (both upper and lower triangles) with adjusted font size
  corrplot(cor_matrix, method = "color", col = col(200),  
           type = "full", order = "original", # Display both triangles
           tl.col = "black", tl.srt = 45, # Text label color and rotation
           addCoef.col = "black",
           tl.cex = 0.7, # Reduce text label size for better readability
           number.cex = 0.5, # Adjust coefficient number size
           na.label = "NA", # Handle NA values in the correlation plot
           diag = FALSE, # Hide the diagonal
           title = paste("Full Correlation Matrix -", dataset_name), # Add title
           mar = c(0, 0, 4, 0)) # Adjust top margin for the title
}
```

```{r}
# plot corr mat for train_filtered_1000m_data:
create_corr_matrix(train_filtered_1000m_data, "1000m Data", numerical_features)
```

Insights from the Correlation Matrix (1000m Data):
1. High Correlation Between Features (Potential Multicollinearity):
* temp and Feels_like (0.99): This strong correlation suggests redundancy. We will consider dropping one, likely feels_like, to reduce multicollinearity.
* Clouds_all and Humidity (0.61): There's moderate correlation, indicating overlap in information. This may require further investigation but isn't critical.

2. Correlation with number_of_pickups:
* Positive Correlation:
- Taxis_pickup_count (0.31) and Total_arrests (0.30) both show moderate positive correlations with Uber pickups, suggesting a link between higher transportation demand and arrest activity.
* Negative Correlation:
- Is_night (-0.50) and Is_weekend (-0.35) show significant drops in pickups during these times, signaling lower demand at night and on weekends.
* Low Correlation:
- Wind_speed (-0.02) and Number_of_persons_injured (0.03) have very low correlations with pickups, suggesting that these features have little impact on Uber demand. These may not be useful predictors in the model.

3. Handling Missing Values (NAs):
* number_of_persons_killed shows missing values (NAs). We will address these missing values late.

4. Key Takeaways: We will focus on reducing redundancy by addressing multicollinearity, especially between temp and feels_like. Features like taxi pickups and total arrests seem to influence Uber demand, while nighttime and weekends are associated with reduced activity. Low correlation features like wind speed and number of persons injured may have little predictive power and can potentially be excluded from the model.

```{r}
# plot corr mat for train_filtered_2000m_data:
create_corr_matrix(train_filtered_2000m_data, "2000m Data", numerical_features)
```

Insights from the Correlation Matrix (2000m Data):
1. High Correlation Between Features (Potential Multicollinearity):
* temp and Feels_like (0.99): This nearly perfect correlation suggests redundancy. We will likely drop feels_like to avoid multicollinearity.
Clouds_all and Humidity (0.61): This moderate correlation indicates some overlap in information, though it is less critical.

2. Correlation with number_of_pickups:
* Positive Correlation:
- Taxis_pickup_count (0.47) and Total_arrests (0.17) show moderate correlations, suggesting that both higher transportation demand (taxi activity) and arrest activity are associated with an increase in Uber pickups.
- Event_count (0.37) and Number_of_crashes (0.37): Events and traffic incidents appear linked to increased demand for Uber pickups, likely due to congestion or disrupted transportation routes.
* Low Correlation:
- Wind_speed (-0.03) and Number_of_persons_killed (0.01) exhibit very weak correlations with pickups, suggesting little to no impact on Uber demand. These features may not be useful predictors.

3. Key Takeaways: We will prioritize reducing multicollinearity by addressing the strong correlation between temp and feels_like. Features like taxi pickups, total arrests, event count, and number of crashes seem to have a clear relationship with Uber demand, making them important predictors. Conversely, low correlation features like wind speed and number of persons killed may not significantly contribute to the model and could be excluded to simplify the analysis.

---------------
What about NAs:
The NA values in the number_of_persons_killed of the correlation matrix occur because the this feature likely contains only a constant value (like all zeros), resulting in a standard deviation of zero. Correlation cannot be computed if there is no variability in the data, which leads to NA in the correlation matrix.

Why This Happens:
Zero Variance: If a feature contains only a single value for all rows (e.g., all zeros or all NAs), its standard deviation is zero. Since correlation measures the co-movement of two variables, the absence of variability makes it impossible to compute correlation.
Constant Feature: If all values in number_of_persons_killed are NA, R cannot compute a correlation, resulting in NA in the correlation matrix.
We checked NA's - The first option sounds reasonable, let's check:
```{r}
# Define a function to check if a column has constant or NA values, and print the dataset name
check_constant_or_na <- function(data, column_name, dataset_name) {
  # Get the unique values from the specified column
  unique_values <- unique(data[[column_name]])
  
  # Output the unique values
  print(paste("Unique values in", column_name, "for", dataset_name, ":"))
  print(unique_values)
  
  # Check if all values are NA or constant
  if (length(unique_values) == 1 && is.na(unique_values)) {
    print(paste("All values in", column_name, "for", dataset_name, "are NA."))
  } else if (length(unique_values) == 1) {
    print(paste("All values in", column_name, "for", dataset_name, "are constant:", unique_values))
  } else {
    print(paste("There is variability in", column_name, "for", dataset_name))
  }
}
```

number_of_persons_killed (this is for only 1000m dataset):
```{r}
# Call the function for 'number_of_persons_killed' in train_filtered_1000m_data
check_constant_or_na(train_filtered_1000m_data, "number_of_persons_killed", "train_filtered_1000m_data")
```

We observed that all values for the 'number_of_persons_killed' feature in the 1000m datasets are constant (0).
Therefore, We will omit this variable later in 1000m data.

Since all the values in the 1000m data for the feature: 'number_of_persons_killed' are 0, we would like to check its behavior in the 2000m data:
```{r}
# Define a function to count and visualize the occurrences for 'number_of_persons_killed'
plot_persons_killed_distribution <- function(data, dataset_name, color) {
  # Create a table of counts for 'number_of_persons_killed'
  counts <- as.data.frame(table(data$number_of_persons_killed))
  
  # Create a bar plot to visualize the distribution
  ggplot(counts, aes(x = Var1, y = Freq)) +
    geom_bar(stat = "identity", fill = color, color = "black") +
    labs(
      title = paste("Distribution of 'number_of_persons_killed' -", dataset_name),
      x = "Number of Persons Killed",
      y = "Count"
    ) +
    theme_minimal()
}
```

```{r}
# Call the function for train_filtered_1000m_data
plot_persons_killed_distribution(train_filtered_1000m_data, "train_filtered_1000m_data", "lightblue")
```
```{r}
# Call the function for train_filtered_2000m_data
plot_persons_killed_distribution(train_filtered_2000m_data, "train_filtered_2000m_data", "lightgreen")
```

We can see that the "number_of_persons_killed" is very negligible in both tables. In the 1000 meter radius table, there are no persons killed at all from April to mid-September, therefore it is very likely that there will be no persons killed until the end of September.
Negligible in the 2000 meter radius table.
In addition, the correlation with the label "number_of_pickups" we calculated earlier for the 2000 table was also very negligible (0.014).
In conclusion, we will remove this feature.

--------
Now, we will take a look on the catagorial features:
We would like to see whether the effect of a categorical feature on the label (number_of_pickups) is strong or weak. We will use Box plot to visually assess the distribution of the label within each category, giving insight into whether the categorical feature significantly impacts the label.

```{r}
# Function to create box plots for each categorical feature
plot_categorical_effect <- function(data, categorical_features, label, dataset_name, fill_color) {
  
  # Loop through each categorical feature
  for (feature in categorical_features) {
    
    # Ensure the feature is treated as a factor (for two levels: 0 and 1)
    data[[feature]] <- as.factor(data[[feature]])
    
    # Create the boxplot for each categorical feature against the label
    p <- ggplot(data, aes_string(x = feature, y = label, fill = feature)) +  # Use fill to color the boxes by feature
      geom_boxplot(color = "black") +
      labs(x = feature, y = label, 
           title = paste("Effect of", feature, "on", label, "-", dataset_name)) +
      scale_fill_manual(values = c(fill_color, fill_color)) +  # Set custom colors for the boxes
      theme_minimal() +
      theme(plot.title = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),
            axis.text.x = element_text(size = 10, angle = 0, hjust = 0.5),  # No rotation needed
            axis.text.y = element_text(size = 10),
            axis.title.x = element_text(size = 12),
            axis.title.y = element_text(size = 12),
            plot.margin = unit(c(1, 1, 1, 1), "cm"))  # Adjust margins
    
    # Print the plot in a new window
    print(p)
  }
}

```

```{r}
plot_categorical_effect(train_filtered_1000m_data, categorical_features, "number_of_pickups", "1000m Data", "lightblue")
```

```{r}
plot_categorical_effect(train_filtered_2000m_data, categorical_features, "number_of_pickups", "2000m Data", "lightgreen")
```

We can see & learn that:
1.is_snowing_last_hour: 
* Has only 0 value in both datasets, this makes sense given the months covered (April to September) are not snowy months in New York City. Therefore, this feature will be omitted for both the 1000m and 2000m datasets.

2. is_weekend:
* In the 1000m data, there is a noticeable decrease in the number of pickups on weekends. Weekdays (is_weekend = 0) show a higher median and wider range of pickups compared to weekends (is_weekend = 1).
* In the 2000m data, the effect of weekends is less pronounced, with very similar medians and distributions for both weekends and weekdays.

3. is_night:
* In the 1000m data, the number of pickups drops significantly at night (is_night = 1). The median and interquartile range (IQR) are lower, indicating reduced Uber demand during nighttime in closer proximity to the Empire State Building.
* In the 2000m data, the nighttime effect is less drastic, though there is still a slight decrease in the number of pickups during the night compared to daytime.

4. is_raining_last_hour: 
* In both 1000m and 2000m data, there is no significant effect of rain on the number of pickups. The medians and IQRs are quite similar whether it is raining or not, suggesting that rain does not strongly affect Uber demand in this dataset.

5. memorial_day, independence_day, and labor_day: 
* In both datasets, we observe fewer pickups on holidays, with lower median pickups on Memorial Day, Independence Day, and Labor Day compared to regular days. However, the imbalance with non-holidays might exaggerate this effect. We will investigate these holidays further to check for behavioral changes in Uber demand and use these variables to identify potential outliers.

-------
Some conclusions so far:
We will remove the features is_snowing_last_hour and number_of_persons_killed from both of the datasets:
```{r}
# Function to remove features from the dataset and update numerical and categorical feature lists
remove_features <- function(data, features_to_remove) {
  
 # Loop through each feature to remove
  for (feature in features_to_remove) {
    
    # Check if the feature is in numerical_features and remove it if present
    if (feature %in% numerical_features) {
      numerical_features <<- setdiff(numerical_features, feature)
    }
    
    # Check if the feature is in categorical_features and remove it if present
    if (feature %in% categorical_features) {
      categorical_features <<- setdiff(categorical_features, feature)
    }
  }
  
  # Remove the specified features from the dataset
  data <- data %>%
    select(-all_of(features_to_remove))
  
  # Return the updated data
  return(data)
}
```

```{r}
features_to_remove <- c("is_snowing_last_hour", "number_of_persons_killed")
```

```{r}
train_filtered_1000m_data <- remove_features(train_filtered_1000m_data, features_to_remove)
train_filtered_1000m_data
```

```{r}
train_filtered_2000m_data <- remove_features(train_filtered_2000m_data, features_to_remove)
train_filtered_2000m_data
```

```{r}
test_fixed_data <- remove_features(test_fixed_data, features_to_remove)
test_fixed_data
```


```{r}
categorical_features
```
```{r}
numerical_features
```

-------
"In this section, we'll perform a multicollinearity check among the numerical features. If any features are highly correlated, we may remove one to prevent redundancy. We'll use the Variance Inflation Factor (VIF) to identify potential multicollinearity issues."

```{r}
# Define a function to fit the linear model and calculate VIF for numerical features only
calculate_vif <- function(data, dataset_name, numerical_features) {
  # Create a formula with only numerical features for VIF calculation
  formula <- as.formula(paste("number_of_pickups ~", paste(numerical_features[-1], collapse = " + ")))
  
  # Fit a linear model to calculate VIF
  model <- lm(formula, data = data)
  
  # Calculate VIF
  vif_values <- vif(model)
  
  # Print the VIF values
  print(paste("VIF values for", dataset_name))
  print(vif_values)
  
  # Check for variables with VIF > 5
  high_vif <- vif_values[vif_values > 5]
  
  cat("\n")
  if (length(high_vif) > 0) {
    print(paste("Features with VIF > 5 for", dataset_name))
    print(high_vif)
  } else {
    print(paste("No features with VIF > 5 for", dataset_name))
  }
}
```


```{r}
# Call the function for train_filtered_1000m_data
calculate_vif(train_filtered_1000m_data, "train_filtered_1000m_data", numerical_features)
```

```{r}
# Call the function for train_filtered_2000m_data
calculate_vif(train_filtered_2000m_data, "train_filtered_2000m_data", numerical_features)
```

We have identified (as we expected) a very high Variance Inflation Factor (VIF) for both temp and feels_like in both datasets, indicating strong multicollinearity between these two variables. This aligns with the high correlation (close to 1) observed earlier between temp and feels_like. To address this, we will drop one of them. In this case, we will drop temp as it shows a slightly weaker correlation with the target variable number_of_pickups, and we will keep feels_like in the model.

Lets run the function to remove this feature:
```{r}
features_to_remove <- c("temp")
```

```{r}
train_filtered_1000m_data <- remove_features(train_filtered_1000m_data, features_to_remove)
train_filtered_1000m_data
```

```{r}
train_filtered_2000m_data <- remove_features(train_filtered_2000m_data, features_to_remove)
train_filtered_2000m_data
```

```{r}
test_fixed_data <- remove_features(test_fixed_data, features_to_remove)
test_fixed_data
```


```{r}
categorical_features
```

```{r}
numerical_features
```

--------

Removing wind_speed, clouds_all and humidity:
1. wind_speed:
* The correlation between wind_speed and number_of_pickups is very low: -0.02 (1000m) and -0.03 (2000m).
* In an urban environment like New York City, wind speed has minimal impact on transportation behavior. The city's infrastructure, such as sheltered streets and extensive public transit, reduces the influence of everyday wind fluctuations on Uber demand.

2. clouds_all:
* The correlation between clouds_all and number_of_pickups is weak: 0.14 in both datasets.
* Cloudiness doesn't significantly impact transportation behavior. Unlike rain or snow, cloud cover alone isn't disruptive enough to influence Uber demand in New York City, making it less relevant for predictive modeling.

3. humidity:
* The correlation between humidity and number_of_pickups is weak: 0.12 (1000m) and 0.11 (2000m).
* Humidity is also a variable that is not expected to directly affect the number of pickups, as there are other, more impactful temperature-related features We prefer to retain. 

Lets run the function to remove those features:
```{r}
features_to_remove <- c("wind_speed", "clouds_all", "humidity")
```

```{r}
train_filtered_1000m_data <- remove_features(train_filtered_1000m_data, features_to_remove)
train_filtered_1000m_data
```

```{r}
train_filtered_2000m_data <- remove_features(train_filtered_2000m_data, features_to_remove)
train_filtered_2000m_data
```

```{r}
test_fixed_data <- remove_features(test_fixed_data, features_to_remove)
test_fixed_data
```

```{r}
categorical_features
```

```{r}
numerical_features
```

#### Descriptive Statistics and Visualization
Now, Lets do some visualizations to understand how the variables behave:
```{r}
# Basic summary statistics - 1000m
summary(train_filtered_1000m_data)
```
```{r}
# Basic summary statistics - 2000m
summary(train_filtered_2000m_data)
```
Key Observations from the 1000m Dataset:
1. Number of Pickups: The median number of pickups is 37, with a maximum of 190. The distribution shows that pickups vary widely in the 1000m radius.
2. Weather: Humidity has a median of 49, and feels_like is around 23.85. Rain is infrequent (mean is 0.16), suggesting rain likely doesn't have a strong influence on demand.
3. Crashes and Injuries: Crashes and injuries are rare in the 1000m area (median values are 0), indicating that incidents might not have a significant effect on Uber demand in this radius.
4. Holidays: Memorial Day, Independence Day, and Labor Day are rare occurrences (mean close to 0), so their effect on demand could be limited.
5. Taxi Pickups and Arrests: Taxi pickups have a median of 641, and total arrests have a median of 34, which indicates moderate activity in both taxi services and law enforcement within the 1000m radius.

Key Observations from the 2000m Dataset:
1. Number of Pickups: The median number of pickups is 204, with a maximum of 521, showing a larger volume of activity in this area.
2. Weather: Humidity has a median of 49, and feels_like is around 23.82. Rain is similarly infrequent (mean of 0.16), likely not a major factor for Uber demand.
3. Crashes and Injuries: There is a higher occurrence of crashes and injuries (median values: 4 crashes, 1 injury), reflecting more frequent incidents in the larger radius.
4. Holidays: Holidays are rare (mean close to 0), and further analysis would be needed to assess their impact on Uber demand.
5. Taxi Pickups and Arrests: Taxi pickups have a median of 641, and total arrests have a median of 985, indicating substantial taxi and law enforcement activity within the 2000m radius.

##### Label & Features Analysis

Distribution of Number of Pickups: We start by visualizing the distribution of the dependent variable (number_of_pickups).
```{r}
# Define a function to create the histogram for the distribution of "number_of_pickups"
plot_pickup_distribution <- function(data, dataset_name, fill_color) {
  ggplot(data, aes(x = number_of_pickups)) +
    geom_histogram(bins = 30, fill = fill_color, color = "black") +
    labs(
      title = paste("Distribution of Number of Pickups -", dataset_name),
      x = "Number of Pickups",
      y = "Count"
    )
}
```

```{r}
# Call the function for train_filtered_1000m_data
plot_pickup_distribution(train_filtered_1000m_data, "train_filtered_1000m_data", "lightblue")
```
Observations from the 1000m Data:
* The distribution of the number of pickups is right-skewed, with the majority of data points concentrated between 0 and 100 pickups.
* A large number of instances have fewer than 50 pickups, indicating that within the 1000m radius of the Empire State Building, lower pickup counts are more frequent.
* There are fewer instances with a high number of pickups (100+), suggesting that higher demand events or locations are less frequent within this smaller radius.

```{r}
# Call the function for train_filtered_2000m_data
plot_pickup_distribution(train_filtered_2000m_data, "train_filtered_2000m_data", "lightgreen")
```
Observations from the 2000m Data:
* The distribution is more symmetric and appears closer to a normal distribution, with most data points falling between 100 and 300 pickups.
* Pickup counts are higher overall, indicating that in the 2000m radius, larger-scale demand for Uber rides is more common.
* There is a wider range of pickups in this dataset, with instances reaching up to 400 pickups, implying a more diverse set of demand situations at this larger distance from the Empire State Building.

##### Trends

Let's look at the trend over different times for the demand for Uber rides in each of the data ranges we have:
```{r}
plot_moving_averages <- function(df, column, data_name) {
  # Calculate moving averages for different trends
  df <- df %>%
    mutate(
      daily_avg = zoo::rollmean(.data[[column]], 28, fill = NA),  # Daily trend (28 intervals in a day)
      weekly_avg = zoo::rollmean(.data[[column]], 196, fill = NA), # Weekly trend (28*7 intervals in a week)
      monthly_avg = zoo::rollmean(.data[[column]], 840, fill = NA) # Monthly trend (28*30 intervals in a month)
    )
  
  # Filter data to start from April
  df <- df %>% filter(time_interval >= as.Date("2014-04-01"))
  
  # Check if the monthly trend starts from May and remove rows before May for monthly trend
  df <- df %>% mutate(monthly_avg = ifelse(time_interval >= as.Date("2014-05-01"), monthly_avg, NA))
  
  # Create a plot
  ggplot(df, aes(x = time_interval)) +
    # Plot the original data in blue
    geom_line(aes(y = .data[[column]], color = "Original"), size = 0.5, alpha = 0.7, linetype = "solid", na.rm = TRUE) +
    
    # Plot the daily moving average in green
    geom_line(aes(y = daily_avg, color = "Daily Trend"), size = 1, linetype = "solid", na.rm = TRUE) +
    
    # Plot the weekly moving average in orange
    geom_line(aes(y = weekly_avg, color = "Weekly Trend"), size = 1, linetype = "solid", na.rm = TRUE) +
    
    # Plot the monthly moving average in red, starting from May
    geom_line(aes(y = monthly_avg, color = "Monthly Trend"), size = 1, linetype = "solid", na.rm = TRUE) +
    
    # Add labels and title
    labs(title = paste("Original vs. Moving Averages (Daily, Weekly, Monthly) for", data_name),
         x = "Time Interval", y = column) +
    
    # Theme adjustments
    theme_minimal() +
    
    # Adding a legend and assigning colors
    scale_color_manual(name = "Trends", 
                       values = c("Original" = "blue", "Daily Trend" = "green", 
                                  "Weekly Trend" = "orange", "Monthly Trend" = "red")) +
    theme(legend.position = "right")  # Position the legend to the right
}

```

```{r}
plot_moving_averages(train_filtered_1000m_data, "number_of_pickups", "1000m Data")
```

* Original Data (Blue): The blue line shows the raw Uber pickup data, with noticeable fluctuations and spikes during peak demand times in 15-minute intervals.

* Daily Trend (Green): The daily moving average smooths out these spikes but still shows some day-to-day variations, keeping demand around 40-50 pickups.

* Weekly Trend (Orange): This trend reveals more stability, showing gradual rises and falls, reflecting week-to-week fluctuations that stay close to the daily averages.

* Monthly Trend (Red): The monthly moving average shows the overall long-term trend, highlighting a steady increase from April to July, followed by a slight decline in August.

* Conclusions from Uber Pickup Data Analysis (1000m Radius around the Empire State Building):
- High Short-Term Volatility: Significant spikes in 15-minute intervals reflect highly dynamic demand, likely driven by tourists, commuters, and events. This indicates the need for responsive driver allocation to handle demand surges.
- Consistent Daily Demand: Despite short-term fluctuations, the daily trend remains stable at around 40-50 pickups per interval. This highlights the area's consistent ride demand, making it a reliable source for Uber activity.
- Predictable Weekly Patterns: Weekly trends show stable fluctuations, suggesting that demand rises and falls predictably over the week, allowing for optimized driver scheduling based on day-to-day variations.
- Seasonal Increase in Summer: The monthly trend indicates rising demand from April to July, peaking with the tourist season, followed by a slight decline in August. This pattern shows how seasonal tourism impacts Uber demand.

* Final Takeaway:
Uber demand in the Empire State area is dynamic but predictable across daily, weekly, and seasonal trends. By recognizing these patterns, Uber can improve driver availability and service efficiency, especially during peak tourist months and high-demand periods.

```{r}
plot_moving_averages(train_filtered_2000m_data, "number_of_pickups", "2000m Data")
```

The graph for the 2000-meter radius around the Empire State Building reveals several key insights into Uber demand:

* Higher Demand Volatility:
Similar to the 1000-meter radius, the original data (blue) shows significant short-term spikes in demand, but at a much larger scale. This area encompasses more of New York City, which could explain the larger fluctuations, reflecting a higher volume of pickups across a broader geographic area.

* Consistent Daily and Weekly Patterns:
The daily (green) and weekly (orange) trends reveal smoother, more stable patterns of demand, with gradual increases and decreases over time. These trends show consistent ride demand day-to-day and week-to-week, with more pronounced peaks during busier times.

* Steady Growth with Monthly Trend:
The monthly trend (red) shows a similar rising pattern from April to July as seen in the 1000-meter graph, but with larger peaks. This indicates that demand grows steadily in the summer months, peaking around July before tapering slightly in August and early September.

* Insights:
- Larger Area, Higher Demand: The increased fluctuations in the 2000-meter radius suggest that covering a broader area increases Uber demand. This makes sense, as this radius covers more tourist spots, business districts, and residential areas.
- Smoother Long-Term Trends: Despite short-term volatility, the long-term trends show steady growth in demand, particularly during peak tourist and summer months.

##### Month, Day, Week

The functions:

Lets look at the sum pickups per month:
Note: We will look only up till the month August because only 9 days are present for the month September in our data.
```{r}
# Function to plot the behavior of a specific feature (e.g., pickups) per month
plot_monthly_behavior <- function(data, feature, dataset_name, color) {
  # Convert time_interval to date format and extract month
  data$month <- format(as.Date(data$time_interval), "%Y-%m")
  
  # Filter the data to end in August
  data <- data %>% filter(as.Date(paste0(month, "-01")) <= as.Date("2014-08-31"))
  
  # Summarize the total of the selected feature per month
  monthly_data <- data %>%
    group_by(month) %>%
    summarise(total_value = sum(!!sym(feature), na.rm = TRUE))  # Use !!sym(feature) to pass the feature name dynamically
  
  # Create a line plot to show the behavior of the selected feature per month
  ggplot(monthly_data, aes(x = month, y = total_value, group = 1)) +
    geom_line(color = color, size = 1) +
    geom_point(color = color, size = 2) +
    labs(
      title = paste("Monthly Behavior of", feature, "-", dataset_name),
      x = "Month",
      y = paste("Total", feature)
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```


Lets look at the sum pickups per week:

Note: Our data ends on 9th of September, which is a Tuesday. This is why we limit the data to the last complete week, ending on Sunday, 09/07/2014. This way, we avoid including any partial weeks, which could distort the weekly trend.By ensuring we only analyze full weeks, we maintain the consistency and accuracy of the weekly behavior analysis.

```{r}
plot_weekly_behavior <- function(data, feature, dataset_name, color) {
  # Convert time_interval to date format and extract the week number and year
  data$week_start <- floor_date(as.Date(data$time_interval), unit = "week", week_start = 1)  # Ensure week starts on Monday
  
  # Filter the data to end on the last full week (before 09/07/2014)
  data <- data %>% filter(week_start <= as.Date("2014-09-07"))
  
  # Summarize the total of the selected feature per week
  weekly_data <- data %>%
    group_by(week_start) %>%
    summarise(total_value = sum(!!sym(feature), na.rm = TRUE)) %>%
    ungroup()  # Ensure ungrouped output
  
  # Create a line plot to show the behavior of the selected feature per week
  ggplot(weekly_data, aes(x = week_start, y = total_value, group = 1)) +
    geom_line(color = color, size = 1) +
    geom_point(color = color, size = 2) +
    labs(
      title = paste("Weekly Behavior of", feature, "-", dataset_name),
      x = "Week",
      y = paste("Total", feature)
    ) +
    scale_x_date(date_labels = "%d %b", date_breaks = "1 week") +  # Adjust x-axis to show dates
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```


Lets look at the sum pickups per day:
```{r}
# Function to plot the behavior of a specific feature (e.g., pickups) per day
plot_daily_behavior <- function(data, feature, dataset_name, color) {
  # Convert time_interval to date format and extract the day
  data$day <- format(as.Date(data$time_interval), "%Y-%m-%d")
  
  # Summarize the total of the selected feature per day
  daily_data <- data %>%
    group_by(day) %>%
    summarise(total_value = sum(!!sym(feature), na.rm = TRUE))  # Use !!sym(feature) to pass the feature name dynamically
  
  # Create a line plot to show the behavior of the selected feature per day
  ggplot(daily_data, aes(x = as.Date(day), y = total_value, group = 1)) +
    geom_line(color = color, size = 1) +
    geom_point(color = color, size = 2) +
    labs(
      title = paste("Daily Behavior of", feature, "-", dataset_name),
      x = "Day",
      y = paste("Total", feature)
    ) +
    theme_minimal() +
    scale_x_date(date_labels = "%b %d", date_breaks = "4 days") +  # Show every 10 days on the x-axis
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Angle the text for readability
}
```

Top n days with the lowest and highest total number of pickups per month:
```{r}
# Function to show top n days with lowest and highest total pickups per month
plot_top_lowest_highest_pickups_by_month <- function(data, feature, color_low, color_high, dataset_name, n) {
  
  # Convert time_interval to Date and Month format
  data <- data %>%
    mutate(
      month = format(as.Date(time_interval), "%Y-%m"),  # Extract year and month
      day = as.Date(time_interval)  # Extract the day only
    )
  
  # Initialize lists to store plots
  lowest_plots <- list()
  highest_plots <- list()
  
  # Loop through each unique month
  for (month_value in unique(data$month)) {
    
    # Filter data for the current month
    monthly_data <- data %>% filter(month == month_value)
    
    # Group by day and calculate the total number of pickups for each day
    daily_summary <- monthly_data %>%
      group_by(day) %>%
      summarise(total_pickups = sum(!!sym(feature), na.rm = TRUE)) %>%
      arrange(total_pickups)  # Sort by total_pickups
    
    # Get the top n days with the lowest pickups
    lowest_n_days <- head(daily_summary, n)
    
    # Get the top n days with the highest pickups
    highest_n_days <- tail(daily_summary, n)
    
    # Create bar plot for the lowest n days
    plot_lowest <- ggplot(lowest_n_days, aes(x = reorder(day, total_pickups), y = total_pickups)) +
      geom_bar(stat = "identity", color = "black", fill = color_low) +
      labs(
        title = paste("Top", n, "Days with Lowest Pickups -", month_value, "-", dataset_name),
        x = "Day",
        y = "Total Pickups"
      ) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    # Create bar plot for the highest n days
    plot_highest <- ggplot(highest_n_days, aes(x = reorder(day, -total_pickups), y = total_pickups)) +
      geom_bar(stat = "identity", color = "black", fill = color_high) +
      labs(
        title = paste("Top", n, "Days with Highest Pickups -", month_value, "-", dataset_name),
        x = "Day",
        y = "Total Pickups"
      ) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    # Add plots to the lists
    lowest_plots[[month_value]] <- plot_lowest
    highest_plots[[month_value]] <- plot_highest
  }
  
  # Return a list of plots for all months
  return(list(lowest = lowest_plots, highest = highest_plots))
}

```

Top n days with the lowest and highest total number of pickups:
```{r}
# Function to show top n days with lowest and highest total number of pickups
plot_top_lowest_highest_pickups <- function(data, feature, color_low, color_high, dataset_name, n) {
  
  # Group by day and calculate the total number of pickups per day
  daily_summary <- data %>%
    group_by(day = as.Date(time_interval)) %>%
    summarise(total_pickups = sum(!!sym(feature), na.rm = TRUE)) %>%
    arrange(total_pickups)  # Sort by total_pickups
  
  # Get the top n days with the lowest pickups
  lowest_n_days <- head(daily_summary, n)
  
  # Get the top n days with the highest pickups
  highest_n_days <- tail(daily_summary, n)
  
  # Create bar plot for the lowest n days
  plot_lowest <- ggplot(lowest_n_days, aes(x = reorder(day, total_pickups), y = total_pickups)) +
    geom_bar(stat = "identity", color = "black", fill = color_low) +
    labs(
      title = paste("Top", n, "Days with Lowest Pickups -", dataset_name),
      x = "Day",
      y = "Total Pickups"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Create bar plot for the highest n days
  plot_highest <- ggplot(highest_n_days, aes(x = reorder(day, -total_pickups), y = total_pickups)) +
    geom_bar(stat = "identity", color = "black", fill = color_high) +
    labs(
      title = paste("Top", n, "Days with Highest Pickups -", dataset_name),
      x = "Day",
      y = "Total Pickups"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  return(list(lowest = plot_lowest, highest = plot_highest))
}
```

----------------------------

For the 1000m data:

month:
```{r}
# Call the function for 1000m data
plot_monthly_behavior(train_filtered_1000m_data, "number_of_pickups", "1000m Data", "lightblue")
```

week:
```{r}
# Call the function for 1000m data
plot_weekly_behavior(train_filtered_1000m_data, "number_of_pickups", "1000m Data", "lightblue")
```

day:
```{r}
# Call the function for 1000m data
plot_daily_behavior(train_filtered_1000m_data, "number_of_pickups", "1000m Data", "lightblue")
```

```{r}
plots <- plot_top_lowest_highest_pickups_by_month(train_filtered_1000m_data, "number_of_pickups", "lightblue", "blue", "1000m Data", 3)
plots
```

```{r}
plots <- plot_top_lowest_highest_pickups(train_filtered_1000m_data, "number_of_pickups", "lightblue", "blue", "1000m Data", 3)
plots
```


The monthly, weekly, and daily graphs reveal notable patterns regarding Uber demand:

* Monthly Behavior:
- There is a clear increase in demand from April to July, peaking in July, followed by a decline in August.
- The peak in July could be associated with the summer months when tourism and activity in New York City are at their highest.

* Weekly Behavior:
- The week starting on June 30th shows a significant drop in Uber pickups, which corresponds with the slight decline seen in the June monthly behavior.
- After this, there is a sharp rise in demand during the week of July 14th, which may contribute to the July peak in the monthly behavior.
- The sharp decline during the week of August 25th likely explains the overall drop observed in August.

* Daily Behavior:
- The daily chart shows a relatively consistent pattern with periodic spikes, indicating some days experience significantly higher demand than others.
- The periodic nature of these spikes might indicate certain recurring events, possibly weekends or holidays, driving these fluctuations.

* Daily Picks: 
- Dates with High Uber Pickups:
1. On May 1, 2014, the Empire State Building celebrated its 83rd anniversary. the Empire State Building often hosts celebrations involving special lighting, VIP tours, or other commemorative activities around May 1, the anniversary date. This is probably the reason for the high demand for Uber on April 30.
2. On July 15, 2014, one major event in New York was Bruno Mars' concert at Madison Square Garden, which was part of his "Moonshine Jungle Tour." This event likely contributed to the spike in Uber pickups, as concerts at such a large venue can draw significant crowds, increasing the need for transportation services. 
3. Regarding September 5, 2014, this date coincides with the US Open Tennis Championships, which take place annually in New York at Flushing Meadows. This major international sporting event attracts tens of thousands of spectators, which would have increased transportation demand, explaining the higher Uber pickup numbers.
4. June 19, or Juneteenth, has been celebrated for years as a significant cultural day marking the emancipation of enslaved African Americans. Even though it only became a federal holiday in 2021, high Uber pickup rates on this date may reflect increased travel to cultural events or gatherings, highlighting how specific days with cultural relevance can temporarily boost local demand. 

- Dates with Low Uber Pickups:
1. On April 6, 2014, New York City hosted several cultural and free events, one of the most notable being the New York Tartan Day Parade, celebrating Scottish heritage with thousands of people lining the streets of Manhattan. The parade featured bagpipers, dancers, and other Scottish cultural displays, which drew large crowds. In addition, there was the ongoing New York International Auto Show, attracting visitors to see the latest trends in the automotive industry. These large-scale events likely influenced transportation patterns in the city, possibly contributing to a reduction in Uber pickups as people opted for public transportation or walked to central event areas.
2. 2014-04-07 - this is fourth of july (Independence Day). Fourth of july in new york 2014: Macy's Fireworks over the East River on Friday, July 4, 2014. Credit: Diana Robinson. We guess people preferred to walk and watch the fireworks.


In summary, the stronger periods for Uber demand are found in the summer months, particularly in July, with sharp drops around late June and late August. The weekly and daily charts provide more detailed insights into these fluctuations, where the drop in late June corresponds to the monthly decline in June, and the spike in July aligns with the overall summer peak.

×‘×¢×‘×¨×™×ª: 
×”×’×¨×¤×™× ×”×—×•×“×©×™×™×, ×”×©×‘×•×¢×™×™× ×•×”×™×•×ž×™×™× ×ž×’×œ×™× ×ª×‘× ×™×•×ª ×ž×¢× ×™×™× ×•×ª ×œ×’×‘×™ ×”×‘×™×§×•×© ×œ××•×‘×¨:

×”×ª× ×”×’×•×ª ×—×•×“×©×™×ª:
×™×© ×¢×œ×™×™×” ×‘×¨×•×¨×” ×‘×‘×™×§×•×© ×ž××¤×¨×™×œ ×•×¢×“ ×™×•×œ×™, ×›×©×”×©×™× ×‘×™×•×œ×™ ×•×œ××—×¨ ×ž×›×Ÿ ×™×¨×™×“×” ×‘××•×’×•×¡×˜.
×”×©×™× ×‘×™×•×œ×™ ×¢×©×•×™ ×œ×”×™×•×ª ×§×©×•×¨ ×œ×¢×•× ×ª ×”×§×™×¥, ×›×©×™×© ×™×•×ª×¨ ×ª×™×™×¨×™× ×•×¤×¢×™×œ×•×ª ×‘×¢×™×¨ × ×™×• ×™×•×¨×§.

×”×ª× ×”×’×•×ª ×©×‘×•×¢×™×ª:
×‘×©×‘×•×¢ ×©×ž×ª×—×™×œ ×‘- 30 ×‘×™×•× ×™ ×™×© ×™×¨×™×“×” ×ž×©×ž×¢×•×ª×™×ª ×‘×ž×¡×¤×¨ ×”×”×–×ž× ×•×ª, ×“×‘×¨ ×©×ž×ª××™× ×œ×™×¨×™×“×” ×”×§×œ×” ×©×¨××™× ×• ×‘×’×¨×£ ×©×œ ×™×•× ×™.
×œ××—×¨ ×ž×›×Ÿ ×™×© ×¢×œ×™×™×” ×—×“×” ×‘×‘×™×§×•×© ×‘×ž×”×œ×š ×”×©×‘×•×¢ ×©×œ 14 ×‘×™×•×œ×™, ×ž×” ×©×ž×¡×‘×™×¨ ××ª ×”×©×™× ×‘×’×¨×£ ×”×—×•×“×©×™ ×©×œ ×™×•×œ×™.
×”×™×¨×™×“×” ×”×—×“×” ×‘×ž×”×œ×š ×”×©×‘×•×¢ ×©×œ 25 ×‘××•×’×•×¡×˜ ×›× ×¨××” ×ž×¡×‘×™×¨×” ××ª ×”×™×¨×™×“×” ×”×›×•×œ×œ×ª ×©× ×¨××ª×” ×‘××•×’×•×¡×˜.

×”×ª× ×”×’×•×ª ×™×•×ž×™×ª:
×”×’×¨×£ ×”×™×•×ž×™ ×ž×¨××” ×“×¤×•×¡ ×§×‘×•×¢ ×™×—×¡×™×ª ×¢× ×§×¤×™×¦×•×ª ×ª×§×•×¤×ª×™×•×ª, ×“×‘×¨ ×©×ž×¨×ž×– ×›×™ ×™×©× × ×™×ž×™× ×©×‘×”× ×”×‘×™×§×•×© ×’×‘×•×” ×ž×©×ž×¢×•×ª×™×ª ×ž××—×¨×™×.
×”×§×¤×™×¦×•×ª ×”×œ×œ×• ×™×›×•×œ×•×ª ×œ×”×¦×‘×™×¢ ×¢×œ ××™×¨×•×¢×™× ×—×•×–×¨×™×, ××•×œ×™ ×¡×•×¤×™ ×©×‘×•×¢ ××• ×—×’×™×, ×©×ž×•×‘×™×œ×™× ×œ×©×™× ×•×™×™× ×‘×‘×™×§×•×©.

×¤×™×§×™× ×™×•×ž×™×:


×œ×¡×™×›×•×, ×”×ª×§×•×¤×•×ª ×”×—×–×§×•×ª ×‘×‘×™×§×•×© ×œ××•×‘×¨ ×”×Ÿ ×‘×—×•×“×©×™ ×”×§×™×¥, ×‘×¢×™×§×¨ ×‘×™×•×œ×™, ×¢× ×™×¨×™×“×•×ª ×—×“×•×ª ×‘×¡×•×£ ×™×•× ×™ ×•×‘×¡×•×£ ××•×’×•×¡×˜. ×”×’×¨×¤×™× ×”×©×‘×•×¢×™×™× ×•×”×™×•×ž×™×™× ×ž×¡×¤×§×™× ×ª×•×‘× ×•×ª ×ž×¤×•×¨×˜×•×ª ×™×•×ª×¨, ×©×‘×”×Ÿ ×”×™×¨×™×“×” ×‘×¡×•×£ ×™×•× ×™ ×§×©×•×¨×” ×œ×™×¨×™×“×” ×‘×™×•× ×™, ×•×”×¢×œ×™×™×” ×‘×™×•×œ×™ ×ª×•××ž×ª ×œ×©×™× ×”×§×™×¥ ×”×›×œ×œ×™.

----------------------------

For the 2000m data:

month:
```{r}
# Call the function for 2000m data
plot_monthly_behavior(train_filtered_2000m_data, "number_of_pickups", "2000m Data", "lightgreen")
```

week:
```{r}
# Call the function for 2000m data
plot_weekly_behavior(train_filtered_2000m_data, "number_of_pickups", "2000m Data", "lightgreen")
```

day:
```{r}
# Call the function for 2000m data
plot_daily_behavior(train_filtered_2000m_data, "number_of_pickups", "2000m Data", "lightgreen")
```

```{r}
plots <- plot_top_lowest_highest_pickups_by_month(train_filtered_2000m_data, "number_of_pickups", "lightgreen", "green", "2000m Data", 3)
plots
```

```{r}
plots <- plot_top_lowest_highest_pickups(train_filtered_2000m_data, "number_of_pickups", "lightgreen", "green", "2000m Data", 3)
plots
```


The monthly, weekly, and daily graphs reveal notable patterns regarding Uber demand within a 2000-meter radius from the Empire State:

* Monthly Behavior: Uber demand shows a gradual increase from April to August, with a peak in August.

* Weekly Behavior: Demand fluctuates, with a clear increase in late July and a sharp rise at the end of August and early September.

* Daily Behavior: There is consistent demand throughout the days, with notable peaks in early September.

* Daily Picks: 
- Dates with High Uber Pickups:
1. September 4-6, 2014: Early September typically sees an increase in New York City activity due to Labor Day weekend events. Celebrations often include concerts, festivals, and major events as the city transitions from summer to fall. Labor Day (September 1 in 2014) also marks a time when tourists and locals alike attend last-minute summer gatherings, boosting transportation needs across the city. Also, September 4: School sessions begin for all students.
2.September 6: The annual NYC Labor Day Parade took place, drawing large crowds to Fifth Avenue. This event celebrates the contributions of organized labor and showcases various unions, creating a vibrant atmosphere with music, banners, and marchers filling the streets. As it marks the end of summer, this parade often brings an uptick in local travel and celebrations.
3. New York Fashion Week 2014 - September 4, 2014 till September 11, 2014.
4. June 19, or Juneteenth (we mwntion before).

- Dates with Low Uber Pickups:
1. April 20-21, 2014: Easter Sunday, which fell on April 20 in 2014, likely reduced demand for Uber services due to family gatherings and church services throughout the day. Many people traditionally spend this holiday with loved ones or attend church services, potentially reducing overall city movement and nightlife activity. (April 20	Easter, April 21	Easter Monday)
2. May 25, 2014: Memorial Day Observed (schools closed).

----------------------------

NOTE:
Many of the events we observed are recurring, expected to happen each year, and typical of a highly touristic city like New York. With numerous significant dates and events, particularly around the Empire State Building, Uber demand naturally fluctuates with these attractions and gatherings. Given this context, we chose not to filter out potential "noise" from the data, as these fluctuations likely reflect genuine patterns in Uber demand.

Additionally, some of our features, such as holidays, weekends, and general events, capture these recurring occasions, providing insight into how they influence demand patterns over time.

----------------------------


```{r}
# Function to plot average pickups per time interval for each day of the week
plot_avg_pickups_by_day_of_week <- function(df, data_name) {
  
  # Ensure the time_interval column is in POSIXct format
  df$time_interval <- as.POSIXct(df$time_interval)
  
  # Extract the day of the week and time part (15-minute intervals)
  df$day_of_week <- weekdays(as.Date(df$time_interval))
  df$time_only <- format(df$time_interval, "%H:%M")
  
  # Group data by day of the week and time interval, and calculate average pickups
  avg_pickups_by_day <- df %>%
    group_by(day_of_week, time_only) %>%
    summarise(avg_pickups = mean(number_of_pickups, na.rm = TRUE), .groups = "drop")
  
  # Define the correct order of time intervals
  time_order <- c('17:00', '17:15', '17:30', '17:45', '18:00', '18:15', '18:30', '18:45',
                  '19:00', '19:15', '19:30', '19:45', '20:00', '20:15', '20:30', '20:45', 
                  '21:00', '21:15', '21:30', '21:45', '22:00', '22:15', '22:30', '22:45', 
                  '23:00', '23:15', '23:30', '23:45', '00:00')
  
  # Order time intervals so that "00:00" comes after "23:45"
  avg_pickups_by_day$time_only <- factor(avg_pickups_by_day$time_only, levels = time_order, ordered = TRUE)
  
  # Define the order of days of the week
  day_order <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
  avg_pickups_by_day$day_of_week <- factor(avg_pickups_by_day$day_of_week, levels = day_order, ordered = TRUE)
  
  # Define colors for each day of the week
  day_colors <- c("Sunday" = "red", "Monday" = "orange", "Tuesday" = "yellow", 
                  "Wednesday" = "green", "Thursday" = "blue", "Friday" = "purple", 
                  "Saturday" = "pink")
  
  # Plot the data
  ggplot(avg_pickups_by_day, aes(x = time_only, y = avg_pickups, color = day_of_week, group = day_of_week)) +
    geom_line(size = 1) + 
    geom_point(size = 2) +
    scale_color_manual(values = day_colors) +
    labs(
      title = paste("Average Uber Pickups by Time Interval and Day of Week -", data_name),
      x = "15-Minute Interval",
      y = "Average Pickups",
      color = "Day of the Week"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
}
```


```{r}
plot_avg_pickups_by_day_of_week(train_filtered_1000m_data, "1000m Data")
```

This visualization displays the average Uber pickups by 15-minute intervals for each day of the week within the 1000-meter radius dataset. Each line represents a different day of the week, showing how demand changes over the course of the evening and early night hours.

Key Insights:
- Higher Demand at the Start of Evenings: All days peak around 17:00-18:00, indicating this is a popular time for Uber pickups, likely due to people finishing work or evening commutes.
- Decline in Demand: As the night progresses, we observe a gradual decline in pickups, with some variations by day.
- Weekday vs. Weekend Patterns: Friday and Saturday tend to sustain higher pickup numbers later into the evening compared to other days, reflecting nightlife and social activity.

This plot provides insights into daily demand patterns, suggesting how Uber demand varies with time and day, highlighting peak and low-demand periods that could guide service allocation or pricing adjustments.


```{r}
plot_avg_pickups_by_day_of_week(train_filtered_2000m_data, "2000m Data")
```

This visualization shows the average Uber pickups by 15-minute intervals for each day of the week within the 2000-meter radius dataset. Each line represents a different day, capturing how demand fluctuates from early evening through late night hours.

Key Observations:
- Peak Evening Demand: Similar to the 1000-meter data, the demand peaks in the early evening, around 18:00, across all days, reflecting end-of-day travel needs.
- Higher Sustained Demand: Unlike the 1000-meter dataset, the 2000-meter radius data maintains a higher level of demand well into the evening, especially on Friday and Saturday nights, indicating an increased use of Uber for social and nightlife activities across a larger area.
- Steady Decline After 20:00: Following the peak, there is a gradual decline in pickups, though Friday and Saturday nights see a less steep drop, suggesting extended demand compared to weekdays.

This plot highlights Uber demand dynamics across a broader area, showing not only the peak times but also sustained demand on weekends, which could help inform broader resource allocation and service adjustments.

##### Taxis

Function that creates a dual-axis plot, where one axis shows the number of Uber pickups and the other shows another feature for each day & week & month:
```{r}
plot_behavior_dual_axis <- function(data, feature, dataset_name, color_pickups, color_feature) {
  
  # Add day, week, and month columns for aggregation
  data$day <- as.Date(data$time_interval)
  data$week_start <- floor_date(as.Date(data$time_interval), unit = "week", week_start = 1)
  data$month <- format(as.Date(data$time_interval), "%Y-%m")
  
  # Filter to include only full weeks and months
  data_week <- data %>% filter(week_start <= as.Date("2014-09-07"))
  data_month <- data %>% filter(as.Date(paste0(month, "-01")) <= as.Date("2014-08-31"))
  
  # Summarize daily, weekly, and monthly data
  daily_data <- data %>%
    group_by(day) %>%
    summarise(
      total_pickups = sum(number_of_pickups, na.rm = TRUE),
      total_feature = sum(!!sym(feature), na.rm = TRUE)
    )
  
  weekly_data <- data_week %>%
    group_by(week_start) %>%
    summarise(
      total_pickups = sum(number_of_pickups, na.rm = TRUE),
      total_feature = sum(!!sym(feature), na.rm = TRUE)
    )
  
  monthly_data <- data_month %>%
    group_by(month) %>%
    summarise(
      total_pickups = sum(number_of_pickups, na.rm = TRUE),
      total_feature = sum(!!sym(feature), na.rm = TRUE)
    )
  
  # Function to create the dual-axis plot with independent scaling
  plot_dual <- function(data, x_col, x_label, title) {
    # Find the range of each feature and pickups to adjust scaling
    max_pickups <- max(data$total_pickups, na.rm = TRUE)
    max_feature <- max(data$total_feature, na.rm = TRUE)
    
    # Scale factor for feature relative to pickups
    scale_factor <- max_pickups / max_feature
    
    ggplot(data, aes(x = !!sym(x_col), group = 1)) +  # Adding group = 1 for proper line connection
      geom_line(aes(y = total_pickups, color = "Total Pickups"), size = 1) +
      geom_point(aes(y = total_pickups, color = "Total Pickups"), size = 2) +
      geom_line(aes(y = total_feature * scale_factor, color = feature), size = 1, linetype = "dashed") +
      geom_point(aes(y = total_feature * scale_factor, color = feature), size = 2) +
      scale_y_continuous(
        name = "Total Number of Pickups",
        sec.axis = sec_axis(~./scale_factor, name = paste("Total", feature))  # Secondary axis for feature
      ) +
      labs(
        title = title,
        x = x_label
      ) +
      scale_color_manual(name = "Legend", 
                         values = setNames(c(color_pickups, color_feature), c("Total Pickups", feature))) +  # Dynamically map feature color
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1),
            axis.title.y.right = element_text(color = color_feature),
            axis.title.y.left = element_text(color = color_pickups),
            legend.position = "right")  # Position legend on the right
  }
  
  # Disable scientific notation for y-axis
  options(scipen = 10)
  
  # Create plots for daily, weekly, and monthly behavior
  daily_plot <- plot_dual(daily_data, "day", "Day", paste("Daily Behavior of Pickups and", feature, "-", dataset_name))
  weekly_plot <- plot_dual(weekly_data, "week_start", "Week", paste("Weekly Behavior of Pickups and", feature, "-", dataset_name))
  monthly_plot <- plot_dual(monthly_data, "month", "Month", paste("Monthly Behavior of Pickups and", feature, "-", dataset_name))
  
  return(list(daily_plot = daily_plot, weekly_plot = weekly_plot, monthly_plot = monthly_plot))
}

```

------------

Lets look at the taxis & uber at the same plot:

for 1000m:
```{r}
plots <- plot_behavior_dual_axis(train_filtered_1000m_data, "taxis_pickup_count", "1000m Data", "lightblue", "pink")
plots
```

for 2000m:
```{r}
plots <- plot_behavior_dual_axis(train_filtered_2000m_data, "taxis_pickup_count", "2000m Data", "lightgreen", "pink")
plots
```

These visualizations offer insight into how Uber and yellow taxi pickups demand across days, weeks, and months in 2014.

Key Takeaways:
- Daily Trends: Uber pickups show a lot of day-to-day variation, likely influenced by specific factors like events or weather, while taxi pickups stay more consistent. It's a bit difficult to understand only from this graph, for this we also added a weekly and daily view.

- Weekly Patterns: On a weekly basis, Uber demand tends to peak and dip, especially around weekends, reflecting its dynamic response to high-demand days. Taxis, while showing some increase during peak times, maintain a steadier demand across the week, which aligns with their widespread availability and established user base.

- Monthly Patterns: Looking at month-to-month changes, both Uber and taxi pickups increase from spring into summer. Uberâ€™s demand shows more noticeable shiftsâ€”likely due to tourism and vacation seasonâ€”while taxis have a more stable monthly trend.

Overall, even though taxis show less dramatic shifts (Probably because in 2014, taxis were more popular and established in New York, so their demand changes tend to be less sharp compared to Uber.), both services tend to follow a similar trend in demand over most periods. This suggests that while Uber and taxis may serve slightly different needs day-to-day, they respond similarly to broader demand cycles in the city.


##### Weekends VS weekdays

Looking at the pickups when marking the weekday & weekend:
```{r}
plot_daily_is_weekend_comparison <- function(data, dataset_name, color) {
  
  # Summarize daily data and group by the is_weekend feature
  daily_data <- data %>%
    group_by(day = as.Date(time_interval)) %>%
    summarise(
      total_pickups = sum(number_of_pickups, na.rm = TRUE),
      is_weekend_value = first(is_weekend)
    )
  
  # Create a plot with dots colored based on is_weekend (0 or 1)
  ggplot(daily_data, aes(x = day, y = total_pickups)) +
    geom_line(color = color, size = 0.8) +  # Line for total pickups
    geom_point(aes(color = factor(is_weekend_value)), size = 3) +  # Color points by is_weekend
    scale_color_manual(
      values = c("0" = "red", "1" = "orange"),
      name = "Weekend Legend",
      labels = c("0" = "Weekday", "1" = "Weekend")
    ) +
    labs(
      title = paste("Daily Uber Demand -", dataset_name),
      x = "Day",
      y = "Total Pickups"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
}
```

Lets keep only the weekday to try and see tendentiousness:
```{r}
plot_daily_is_weekday_comparison <- function(data, dataset_name, color) {
  
  # Summarize daily data and group by the is_weekend feature
  daily_data <- data %>%
    group_by(day = as.Date(time_interval)) %>%
    summarise(
      total_pickups = sum(number_of_pickups, na.rm = TRUE),
      is_weekend_value = first(is_weekend)
    )
  
  # Filter out weekend days (is_weekend = 1)
  daily_data <- daily_data %>% filter(is_weekend_value == 0)
  
  # Create a plot with dots colored based on is_weekend (0 or 1)
  ggplot(daily_data, aes(x = day, y = total_pickups)) +
    geom_line(color = color, size = 0.8) +  # Line for total pickups
    geom_point(aes(color = factor(is_weekend_value)), size = 3) +  # Color points by is_weekend
    scale_color_manual(
      values = c("0" = "red"),
      name = "Weekend Legend",
      labels = c("0" = "Weekday")
    ) +
    labs(
      title = paste("Daily Uber Demand -", dataset_name),
      x = "Day",
      y = "Total Pickups"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
}
```

Plotting the average pickups for each time interval, separated by weekdays and weekends:
```{r}
# Function to plot average Uber demand for each time interval, separated by weekdays and weekends
plot_avg_pickups_by_interval <- function(data, dataset_name) {
  
  # Extract the time part of the time_interval for grouping by 15-minute intervals
  data$time_only <- format(as.POSIXct(data$time_interval), "%H:%M")
  
  # Define the correct order of time intervals with "00:00" at the end
  time_order <- c('17:00', '17:15', '17:30', '17:45', '18:00', '18:15', '18:30', '18:45',
                  '19:00', '19:15', '19:30', '19:45', '20:00', '20:15', '20:30', '20:45', 
                  '21:00', '21:15', '21:30', '21:45', '22:00', '22:15', '22:30', '22:45', 
                  '23:00', '23:15', '23:30', '23:45', '00:00')
  
  # Calculate the average pickups for weekdays
  weekday_data <- data %>%
    filter(is_weekend == 0) %>%  # Filter for weekdays
    group_by(time_only) %>%
    summarise(avg_pickups = mean(number_of_pickups, na.rm = TRUE)) %>%
    arrange(factor(time_only, levels = time_order))  # Arrange by time_order
  
  # Calculate the average pickups for weekends
  weekend_data <- data %>%
    filter(is_weekend == 1) %>%  # Filter for weekends
    group_by(time_only) %>%
    summarise(avg_pickups = mean(number_of_pickups, na.rm = TRUE)) %>%
    arrange(factor(time_only, levels = time_order))  # Arrange by time_order
  
  # Plot function for both weekday and weekend
  plot_avg_pickups <- function(data, title, color) {
    ggplot(data, aes(x = factor(time_only, levels = time_order), y = avg_pickups, group = 1)) +  # Use ordered factor
      geom_line(color = color, size = 1) +  # Line for avg pickups
      geom_point(color = color, size = 3) +  # Points for avg pickups
      labs(
        title = title,
        x = "Time Interval",
        y = "Average Pickups"
      ) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
  }
  
  # Create plots for weekdays and weekends
  weekday_plot <- plot_avg_pickups(weekday_data, paste("Average Pickups by Time Interval - Weekdays -", dataset_name), "red")
  weekend_plot <- plot_avg_pickups(weekend_data, paste("Average Pickups by Time Interval - Weekends -", dataset_name), "orange")
  
  return(list(weekday_plot = weekday_plot, weekend_plot = weekend_plot))
}

```


We would like to understand if weekends have an effect on demand for Uber:

for 1000m:
```{r}
plot_daily_is_weekend_comparison(train_filtered_1000m_data, "1000m Data", "lightblue")
```

```{r}
plot_daily_is_weekday_comparison(train_filtered_1000m_data, "1000m Data", "lightblue")
```

```{r}
plot_avg_pickups_by_interval(train_filtered_1000m_data, "1000m Data")
```
The comparison between weekday and weekend Uber demand for the 1000m radius highlights some distinct usage patterns:

Weekdays: Demand is higher, with a clear peak around 17:45, likely reflecting evening rush hour as people finish work. The pickups steadily decline into the evening, showing a gradual decrease as the night progresses.

Weekends: Demand is notably lower, with a smaller peak early in the evening but no consistent downward trend. Instead, the pattern shows small fluctuations through the evening, suggesting more casual, irregular trips that may be related to social outings or leisure activities.

Overall, Uber demand on weekdays is structured around routine activities, while weekend demand is lower and more varied. This pattern aligns with the idea that weekday demand is more predictable due to commuting needs, while weekend demand is more dispersed and less affected by specific peak hours.

for 2000m:
```{r}
plot_daily_is_weekend_comparison(train_filtered_2000m_data, "2000m Data", "lightgreen")
```

```{r}
plot_daily_is_weekday_comparison(train_filtered_2000m_data, "2000m Data", "lightgreen")
```

```{r}
plot_avg_pickups_by_interval(train_filtered_2000m_data, "2000m Data")
```


For the 2000m data, the weekday versus weekend demand patterns show distinct trends.

Daily Demand Patterns:

Weekdays generally exhibit a more stable and consistently high demand throughout the observed period, with pickups spread across each day.
Weekends show a slightly more scattered demand pattern, with demand generally lower than on weekdays, but with occasional peaks that align with events or specific weekend activities.
Hourly Demand Trends:

During weekdays, Uber pickups start at a high level in the early evening (around 17:00) and remain relatively stable until gradually decreasing after 21:00, reaching a low point by midnight.
On weekends, while the pickup rate also starts high in the early evening, thereâ€™s a more noticeable dip and a fluctuating pattern across different intervals, likely due to varied social activities and outing habits on weekends.

In summary, the 2000m data suggests that weekdays maintain a more consistent Uber demand in the evenings, while weekends experience more varied demand. This difference likely reflects more routine weekday commuting compared to the variable social and leisure activities typical of weekends.

---------------------

The differences between the 1000m and 2000m datasets reveal unique demand patterns around the Empire State Building.

1000m Radius: This area, tightly focused around the Empire State Building, shows sharper peaks and troughs in Uber demand, especially during rush hours. Weekday pickups are high, likely driven by commuters and tourists in the central Manhattan area. Weekends see less demand, with more variability and lower averages.

2000m Radius: Extending beyond the immediate vicinity of the Empire State, this larger area captures a broader range of activities across parts of New York. Demand here is generally more stable, reflecting a mix of residential, business, and event-driven pickups. Weekday demand stays steady with a gradual evening decline, while weekends show less consistency but still maintain a solid baseline, likely due to broader social and leisure activities spread across the larger radius.

In summary, the 1000m data is more influenced by the Empire Stateâ€™s immediate, high-traffic area, while the 2000m data reflects a wider mix of urban dynamics, smoothing out peaks and showing consistent demand across a broader area.

×”×”×‘×“×œ×™× ×‘×™×Ÿ ×”× ×ª×•× ×™× ×©×œ ×¨×“×™×•×¡ 1000 ×ž×˜×¨ ×¡×‘×™×‘ ×”××ž×¤×™×™×¨ ×¡×˜×™×™×˜ ×œ×‘×™×Ÿ ×¨×“×™×•×¡ 2000 ×ž×˜×¨ ×ž×©×§×¤×™× ×“×¤×•×¡×™ ×‘×™×§×•×© ×©×•× ×™×.

×¨×“×™×•×¡ 1000 ×ž×˜×¨: ××–×•×¨ ×–×”, ×©×ž×ž×•×§×“ ×¡×‘×™×‘ ×”××ž×¤×™×™×¨ ×¡×˜×™×™×˜, ×ž×¦×™×’ ×¤×™×§×™× ×—×“×™× ×™×•×ª×¨ ×‘×©×¢×•×ª ×”×©×™×, ×‘×ž×™×•×—×“ ×‘×ž×”×œ×š ×™×ž×™ ×—×•×œ. ×›×ž×•×ª ×”× ×¡×™×¢×•×ª ×’×‘×•×”×” ×‘×ž×™×•×—×“ ×‘×™×ž×™ ×—×•×œ, ×›×›×œ ×”× ×¨××” ×‘×¢×§×‘×•×ª × ×•×¡×¢×™× ×•×¢×•×‘×“×™× ×©× ×¢×™× ×‘××–×•×¨ ×ž×¨×›×– ×ž× ×”×˜×Ÿ. ×‘×¡×•×¤×™ ×©×‘×•×¢, ×”×‘×™×§×•×© ×™×•×¨×“ ×•× ×¢×©×” ×¤×—×•×ª ×™×¦×™×‘.

×¨×“×™×•×¡ 2000 ×ž×˜×¨: ×”×¨×“×™×•×¡ ×”×ž×•×¨×—×‘ ×—×•×¨×’ ×ž×”×¡×‘×™×‘×” ×”×ž×™×™×“×™×ª ×©×œ ×”××ž×¤×™×™×¨ ×¡×˜×™×™×˜ ×•×œ×•×›×“ ×ª× ×•×¢×” ×¨×—×‘×” ×™×•×ª×¨, ×›×•×œ×œ ×—×œ×§×™× ×ž× ×™×• ×™×•×¨×§. ×”×‘×™×§×•×© ×›××Ÿ ×™×¦×™×‘ ×™×•×ª×¨ ×•×ž×©×§×£ ×©×™×œ×•×‘ ×©×œ ×ž×’×•×¨×™×, ×¢×¡×§×™× ×•××™×¨×•×¢×™× ×‘×¨×—×‘×™ ×”××–×•×¨ ×”×¨×—×‘ ×™×•×ª×¨. ×‘×™×ž×™ ×—×•×œ, ×”×‘×™×§×•×© × ×•×ª×¨ ×™×—×¡×™×ª ×§×‘×•×¢ ×¢× ×™×¨×™×“×” ×”×“×¨×’×ª×™×ª ×‘×¢×¨×‘, ×‘×¢×•×“ ×©×‘×¡×•×¤×™ ×©×‘×•×¢ ×™×© ×¤×—×•×ª ×¢×§×‘×™×•×ª ××‘×œ ×¢×“×™×™×Ÿ ×©×ž×™×¨×” ×¢×œ ×¨×ž×” ×™×¦×™×‘×”, ×›×›×œ ×”× ×¨××” ×¢×§×‘ ×¤×¢×™×œ×•×™×•×ª ×—×‘×¨×ª×™×•×ª ×•×¤× ××™.

×‘×§×™×¦×•×¨, ×”× ×ª×•× ×™× ×©×œ 1000 ×ž×˜×¨ ×ž×©×§×¤×™× ×™×•×ª×¨ ××ª ×”×ª× ×•×¢×” ×¡×‘×™×‘ ××–×•×¨ ×”××ž×¤×™×™×¨ ×¡×˜×™×™×˜ ×”×ž×¨×›×–×™ ×•×”×¢×ž×•×¡, ×‘×¢×•×“ ×©×”× ×ª×•× ×™× ×©×œ 2000 ×ž×˜×¨ ×ž×©×§×¤×™× ××ª ×”×“×™× ×ž×™×§×” ×”×¢×™×¨×•× ×™×ª ×”×¨×—×‘×” ×™×•×ª×¨, ×•×ž×¨××™× ×‘×™×§×•×© ××—×™×“ ×œ××•×¨×š ×›×œ ×”××–×•×¨.


##### Evening hours vs night hours

What are the pick hours?
Lets look at the 15 min interval and their sum of pickups from all days
```{r}
create_bin_plot <- function(df, data_name) {
  
  # Ensure that time_interval is in POSIXct format
  df$time_interval <- as.POSIXct(df$time_interval)
  
  # Extract the time part from time_interval
  df$time_only <- format(df$time_interval, "%H:%M")
  
  # Check if the 'number_of_pickups' column exists and sum pickups by time interval
  if("number_of_pickups" %in% colnames(df)) {
    grouped_df <- df %>%
      group_by(time_only) %>%
      summarise(number_of_pickups = sum(number_of_pickups, na.rm = TRUE))
  } else {
    stop("The 'number_of_pickups' column was not found in the dataset.")
  }
  
  # Define the evening and night intervals (including "00:00")
  evening_intervals <- c('17:00', '17:15', '17:30', '17:45', '18:00', '18:15', '18:30', '18:45',
                         '19:00', '19:15', '19:30', '19:45', '20:00', '20:15', '20:30')
  night_intervals <- c('20:45', '21:00', '21:15', '21:30', '21:45', '22:00', '22:15', '22:30', 
                       '22:45', '23:00', '23:15', '23:30', '23:45', '00:00')
  
  # Assign categories for legend
  grouped_df$category <- ifelse(grouped_df$time_only %in% evening_intervals, "Evening", 
                                ifelse(grouped_df$time_only %in% night_intervals, "Night", "Other"))
  
  # Order time intervals so that "00:00" comes after "23:45"
  time_order <- c(evening_intervals, night_intervals)
  grouped_df$time_only <- factor(grouped_df$time_only, levels = time_order, ordered = TRUE)
  
  # Create the bin plot with legend for colors
  ggplot(grouped_df, aes(x = time_only, y = number_of_pickups, fill = category)) +
    geom_bar(stat = "identity") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
    scale_fill_manual(values = c("Evening" = "purple", "Night" = "yellow")) +
    labs(title = paste("Uber Pickups by Time Interval -", data_name),
         x = "Time Interval", y = "Total Pickups", fill = "Category")
}
```

Function that will create a table displaying the sum of pickups for each 15min interval:
```{r}
create_interval_table <- function(df) {
  
  # Ensure that time_interval is in POSIXct format
  df$time_interval <- as.POSIXct(df$time_interval)
  
  # Extract the time part from time_interval
  df$time_only <- format(df$time_interval, "%H:%M")
  
  # Summarize the total pickups by 15-minute time interval
  interval_summary <- df %>%
    group_by(time_only) %>%
    summarise(total_pickups = sum(number_of_pickups, na.rm = TRUE))
  
  # Rename the columns for clarity
  interval_summary <- interval_summary %>%
    rename(`15_min_interval` = time_only, `Sum_of_Pickups` = total_pickups)
  
  # Sort the summary table by Sum_of_Pickups in descending order
  interval_summary <- interval_summary %>%
    arrange(desc(Sum_of_Pickups))
  
  # Return the sorted summary table
  return(interval_summary)
}

```


for 1000m:
```{r}
create_bin_plot(train_filtered_1000m_data, "1000m Data")
```

```{r}
interval_table <- create_interval_table(train_filtered_1000m_data)
print(interval_table)
```


for 2000m:
```{r}
create_bin_plot(train_filtered_2000m_data, "2000m Data")
```

```{r}
interval_table <- create_interval_table(train_filtered_2000m_data)
print(interval_table)
```

The bar charts highlight Uber demand patterns in different time intervals within 1000 meters and 2000 meters of the Empire State Building.

For the 1000m radius data, the demand peaks between 17:00 and 19:00 in the evening, with a gradual decline as the night progresses. This trend shows that Uber is most frequently used around the end of the workday and early evening, likely for commuting purposes, and tapers off later in the night.

In the 2000m radius, covering a broader area, the pattern is similar but with significantly higher total pickups. Demand remains higher across more evening intervals, suggesting more dispersed pickup locations and a wider variety of trip purposes. The broader area sustains higher volumes even into the night, reflecting the diverse nightlife, dining, and entertainment options that attract users well into the late hours.

Both graphs reveal the distinction between evening and night demand, with the 2000m radius maintaining higher activity levels than the more localized 1000m radius.

----------------------------

We saw before that for both of that data-sets the week starting at June 30, 2014 had a drop in demand for Uber, we'll take a closer look at it:
```{r}
# Function to create a plot for each day of the week, with dates next to the day names
plot_weekly_uber_pickups <- function(df, data_name, first_day_of_week) {
  
  # Convert first day of the week to date
  first_day_of_week <- as.Date(first_day_of_week)
  
  # Create a sequence of 7 days starting from the given first day
  days_of_week <- seq.Date(from = first_day_of_week, by = "day", length.out = 7)
  
  # Filter data for the specified week
  weekly_data <- df %>%
    filter(as.Date(time_interval) %in% days_of_week)
  
  # Extract time part from time_interval
  weekly_data$time_only <- format(as.POSIXct(weekly_data$time_interval), "%H:%M")
  
  # Group data by day and 15-minute intervals
  weekly_summary <- weekly_data %>%
    group_by(day = as.Date(time_interval), time_only) %>%
    summarise(total_pickups = sum(number_of_pickups, na.rm = TRUE), .groups = 'drop')
  
  # Define base day labels
  day_labels <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
  
  # Add dates to the day labels
  day_labels_with_dates <- paste(day_labels, format(days_of_week, "%Y-%m-%d"), sep = ": ")
  
  # Create a mapping of days_of_week to day_labels_with_dates
  day_map <- setNames(day_labels_with_dates, as.character(days_of_week))
  
  # Add day labels to weekly_summary
  weekly_summary$day_label <- day_map[as.character(weekly_summary$day)]
  
  # Define colors for each day (now corresponding to both the day and date)
  day_colors_with_dates <- setNames(c("red", "orange", "yellow", "green", "blue", "purple", "pink"), day_labels_with_dates)
  
  # Order the time intervals
  time_order <- c('17:00', '17:15', '17:30', '17:45', '18:00', '18:15', '18:30', '18:45',
                  '19:00', '19:15', '19:30', '19:45', '20:00', '20:15', '20:30', '20:45', 
                  '21:00', '21:15', '21:30', '21:45', '22:00', '22:15', '22:30', '22:45', 
                  '23:00', '23:15', '23:30', '23:45', '00:00')

  weekly_summary$time_only <- factor(weekly_summary$time_only, levels = time_order, ordered = TRUE)
  
  # Plot the data
  ggplot(weekly_summary, aes(x = time_only, y = total_pickups, color = day_label, group = day_label)) +
    geom_line(size = 1) + 
    geom_point(size = 2) +
    scale_color_manual(values = day_colors_with_dates) +
    labs(
      title = paste("Uber Pickups Behavior by Day of Week ",first_day_of_week, "for: ", data_name),
      x = "15-Minute Interval",
      y = "Total Pickups",
      color = "Day of the Week"
    ) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Rotate x-axis labels for better readability
}
```

```{r}
plot_weekly_uber_pickups(train_filtered_1000m_data, "1000m Data", "2014-06-30")
```


```{r}
plot_weekly_uber_pickups(train_filtered_2000m_data, "2000m Data", "2014-06-30")
```

On July 2, 2014 in New York:
Billy Joel Concert - Billy Joel performed at Madison Square Garden as part of his monthly residency at the venue. The setlist included many of his popular hits like "Piano Man" and "New York State of Mind," drawing a large crowd of fans.


##### Arrest

for 1000:
```{r}
plots <- plot_behavior_dual_axis(train_filtered_1000m_data, "total_arrests", "1000m Data", "lightblue", "pink")
plots
```


for 2000:
```{r}
plots <- plot_behavior_dual_axis(train_filtered_2000m_data, "total_arrests", "2000m Data", "lightgreen", "pink")
plots
```

There seems to be a noticeable correlation between the number of arrests and Uber demand. A likely reason for this is that some arrests might be due to traffic-related incidents, like reckless driving or hit-and-runs, which can mess with traffic flow. When roads are blocked or detours pop up, people may lean toward using Uber instead of dealing with the hassle of driving themselves.

On the other side, if these incidents are serious enough to close off key streets or make areas hard to reach by car, it might force people to switch to other options like walking or using the subway, which could bring down Uber demand.


##### Rain

```{r}
# Function to calculate and print the percentage of intervals with rain vs no rain
calculate_rain_percentage <- function(df) {
  
  # Check if 'is_raining_last_hour' column exists in the data frame
  if (!"is_raining_last_hour" %in% colnames(df)) {
    stop("Error: 'is_raining_last_hour' column is not found in the data frame.")
  }
  
  # Calculate the total intervals and count of intervals with rain and no rain
  total_intervals <- nrow(df)
  rain_intervals <- sum(df$is_raining_last_hour == 1, na.rm = TRUE)
  no_rain_intervals <- sum(df$is_raining_last_hour == 0, na.rm = TRUE)
  
  # Calculate the percentages
  rain_percentage <- (rain_intervals / total_intervals) * 100
  no_rain_percentage <- (no_rain_intervals / total_intervals) * 100
  
  # Print the results
  cat("Percentage of intervals with rain:", round(rain_percentage, 2), "%\n")
  cat("Percentage of intervals with no rain:", round(no_rain_percentage, 2), "%\n")
}
```

```{r}
calculate_rain_percentage(train_filtered_1000m_data)
```

```{r}
calculate_rain_percentage(train_filtered_2000m_data)
```

This will plot Uber demand for the specified time interval on weekdays, with color-coded dots indicating whether it was raining during the last hour:

```{r}
# Function to create a daily plot for a given time interval, showing only weekdays and color dots by rain
plot_uber_demand_for_time_interval <- function(data, dataset_name, time_interval_string, color) {
  
  # Ensure the time_interval column in the data is in POSIXct format
  data <- data %>%
    mutate(
      time_interval = as.POSIXct(time_interval),  # Convert to POSIXct format if necessary
      time_only = format(time_interval, "%H:%M"),  # Extract the time portion
      day = as.Date(time_interval)  # Extract the day portion
    )
  
  # Filter the data for the specific time interval and for weekdays (is_weekend = 0)
  interval_data <- data %>%
    filter(time_only == time_interval_string, is_weekend == 0)
  
  # Summarize daily data for the given time interval
  daily_summary <- interval_data %>%
    group_by(day) %>%
    summarise(
      total_pickups = sum(number_of_pickups, na.rm = TRUE),
      is_raining_last_hour_value = first(is_raining_last_hour)
    )
  
  # Create the plot with color-coded dots for rain status
  plot <- ggplot(daily_summary, aes(x = day, y = total_pickups)) +
    geom_line(color = color, size = 0.8) +  # Line for total pickups
    geom_point(aes(color = factor(is_raining_last_hour_value)), size = 3) +  # Color points by rain status
    scale_color_manual(
      values = c("0" = "yellow", "1" = "pink"),
      name = "Rain Status",
      labels = c("0" = "No Rain", "1" = "Raining")
    ) +
    labs(
      title = paste("Uber Demand for", time_interval_string, "on Weekdays -", dataset_name),
      x = "Day",
      y = "Total Pickups"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
  
  return(plot)
}

```


for 1000m:
```{r}
plot_uber_demand_for_time_interval(train_filtered_1000m_data, "1000m Data", "17:45", "lightblue")
```

```{r}
plot_uber_demand_for_time_interval(train_filtered_1000m_data, "1000m Data", "20:30", "lightblue")
```

```{r}
plot_uber_demand_for_time_interval(train_filtered_1000m_data, "1000m Data", "22:30", "lightblue")
```

```{r}
plot_uber_demand_for_time_interval(train_filtered_1000m_data, "1000m Data", "19:30", "lightblue")
```


for 2000m:
```{r}
plot_uber_demand_for_time_interval(train_filtered_2000m_data, "2000m Data", "18:00", "lightgreen")
```

```{r}
plot_uber_demand_for_time_interval(train_filtered_2000m_data, "2000m Data", "20:30", "lightgreen")
```

```{r}
plot_uber_demand_for_time_interval(train_filtered_2000m_data, "2000m Data", "22:30", "lightgreen")
```


```{r}
plot_uber_demand_for_time_interval(train_filtered_2000m_data, "2000m Data", "19:30", "lightgreen")
```


We saw earlier that the peak hours for Data 1000 is 17:45 and for Data 2000 it is 18:00 (hours will the biggest amount of uber pickups overall).
During rush hour (17:45 for 1000m and 18:00 for 2000m), people tend to take Uber regardless of whether it is raining or not. This is likely due to the necessity of commuting at those times. However, we see similar trends in pickups, where rain does not seem to significantly alter the demand pattern. This is expected during rush hours when people prioritize getting home or to work, rain or shine.

To better understand the effect of rain, we should consider looking at other time intervals where Uber demand is not the highest. By choosing a time that is not during rush hour but also not too late, we can see whether rain significantly influences Uber pickups when commuters have more flexibility in their transportation choices.

We focused specifically on weekdays rather than weekends to more clearly observe the effect of rain. When we look at times like 20:30 and 19:30, itâ€™s evident that many of the rainy days had higher Uber demand compared to other days at the same time (though other factors also influence Uber demand). This aligns with the idea that, on rainy days, people may prefer Uber to avoid getting wet, as walking or using public transportation might not be as convenient.

In later evening hours, when people are more likely to go out for leisure, the effect of rain becomes more variable. On rainy evenings, some people might choose to stay home, leading to lower Uber demand. However, for those who do decide to go out, rain could increase the likelihood of using Uber instead of walking or waiting for other forms of transportation. So, rain can lead to inconsistent demand in these later hoursâ€”either boosting Uber pickups if people still go out or reducing them if the rain discourages people from leaving their homes.


##### Holidays

Now we will look at the days when there were holidays and see their behavior regarding Uber demand.
Are there changes in demand? Do all holidays behave the same?

```{r}
# Function to plot daily Uber demand with coloring by holidays, handling NAs
plot_daily_holiday_comparison <- function(df, data_name, color) {
  
  # Convert time_interval to date format for grouping by day
  df$day <- as.Date(df$time_interval)
  
  # Summarize daily data and check if any holiday applies
  daily_data <- df %>%
    group_by(day) %>%
    summarise(
      total_pickups = sum(number_of_pickups, na.rm = TRUE),
      is_labor_day = max(labor_day, na.rm = TRUE),
      is_independence_day = max(independence_day, na.rm = TRUE),
      is_memorial_day = max(memorial_day, na.rm = TRUE)
    ) %>%
    # Handle cases where all holidays are NA or 0
    mutate(holiday = case_when(
      is_labor_day == 1 ~ "Labor Day",
      is_independence_day == 1 ~ "Independence Day",
      is_memorial_day == 1 ~ "Memorial Day",
      TRUE ~ NA_character_  # Keep NA for no holiday
    ))
  
  # Plot daily Uber demand with points colored based on holidays
  ggplot(daily_data, aes(x = day, y = total_pickups)) +
    geom_line(color = color, size = 0.8) +  # Line for total pickups
    geom_point(aes(color = holiday), size = 3, show.legend = TRUE, na.rm = TRUE) +  # Color points by holiday, remove NA values from points
    scale_color_manual(
      values = c("Labor Day" = "red", 
                 "Independence Day" = "orange", 
                 "Memorial Day" = "purple"),
      na.translate = FALSE  # Hide legend entry for NA values
    ) +
    labs(
      title = paste("Daily Uber Demand with Holiday Highlight -", data_name),
      x = "Day",
      y = "Total Pickups",
      color = "Holiday"
    ) +
    theme_minimal() +
    scale_x_date(date_breaks = "7 days", date_labels = "%b %d") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels
}
```


for 1000m:
```{r}
plot_daily_holiday_comparison(train_filtered_1000m_data, "1000m Data", "lightblue")
```

for 2000m:
```{r}
plot_daily_holiday_comparison(train_filtered_2000m_data, "2000m Data", "lightgreen")
```

We have observed a consistent pattern across the three holidaysâ€”Labor Day, Independence Day, and Memorial Dayâ€”where Uber demand is significantly lower compared to non-holiday days. This similarity in behavior suggests that it would be beneficial to consolidate these holidays into a single feature, is_holiday, rather than keeping them as separate variables. This simplification will help reduce complexity in the model without losing the insight that holidays impact demand.

This trend makes sense because, during holidays, fewer people commute to work, which naturally reduces the need for Uber rides. Additionally, many people prefer to stay at home or spend time with their families, further contributing to the lower demand for rides. By creating the is_holiday feature, we capture this common effect more efficiently, reflecting the societal behavior during holidays.

```{r}
# Function to create is_holiday feature and remove original holiday columns
create_is_holiday_feature <- function(df) {
  
  # Create the new is_holiday feature
  df$is_holiday <- ifelse(df$labor_day == 1 | df$independence_day == 1 | df$memorial_day == 1, 1, 0)
  
  # Remove the original holiday columns
  features_to_remove <- c("labor_day", "independence_day", "memorial_day")
  
  df <- df %>%
    select(-all_of(features_to_remove))

  return(df)  # Return the updated dataframe
}
```


##### Crashes & Injured

Here we have info about: number_of_persons_injured & number_of_crashes
Let's see if these parameters have an effect on the demand for Uber:

all the unique val is number_of_persons_injured:
for 1000m:
```{r}
unique_vals_1000 <- unique(train_filtered_1000m_data$number_of_persons_injured)
print(unique_vals_1000)
```

for 2000m:
```{r}
unique_vals_2000 <- unique(train_filtered_2000m_data$number_of_persons_injured)
print(unique_vals_2000)
```
all the unique val is number_of_crashes:

for 1000m:
```{r}
unique_vals_1000 <- unique(train_filtered_1000m_data$number_of_crashes)
print(unique_vals_1000)
```

for 2000m:
```{r}
unique_vals_2000 <- unique(train_filtered_2000m_data$number_of_crashes)
print(unique_vals_2000)
```

-------------

Here's a function that prints the date of the day with the most from a certain from the, along with the sum of that certain feature on that day:
```{r}
# Function to print the day with the most of a certain feature
print_max_feature_day <- function(data, feature) {
  
  # Ensure the time_interval is in POSIXct format and extract the day
  data$day <- as.Date(data$time_interval)
  
  # Summarize the total values of the feature by day
  day_summary <- data %>%
    group_by(day) %>%
    summarise(total_feature_value = sum(!!sym(feature), na.rm = TRUE)) %>%
    ungroup()  # Ungroup to avoid grouping issues later
  
  # Find the day with the maximum feature value
  max_day_info <- day_summary %>%
    filter(total_feature_value == max(total_feature_value, na.rm = TRUE))
  
  # Extract the date and max value
  max_day <- as.Date(max_day_info$day)  # Ensuring it's in date format
  max_value <- max_day_info$total_feature_value
  
  # Print the result
  cat("The day with the most", feature, "is:", format(max_day, "%Y-%m-%d"), 
      "with a sum of", feature, ":", max_value, "\n")
}

```

------------


For 1000:
```{r}
print_max_feature_day(train_filtered_1000m_data, "number_of_crashes")
```

```{r}
print_max_feature_day(train_filtered_1000m_data, "number_of_persons_injured")
```

```{r}
# Function to plot the histogram of a numerical feature
plot_histogram <- function(df, data_name, feature, color) {
  
  # Ensure the feature exists in the dataframe
  if (!feature %in% colnames(df)) {
    stop(paste("Feature", feature, "not found in the dataframe."))
  }
  
  # Plot the histogram of the selected feature
  ggplot(df, aes_string(x = feature)) +
    geom_histogram(bins = 30, fill = color, color = "black", alpha = 0.7) +
    labs(
      title = paste("Histogram of", feature, "-", data_name),
      x = feature,
      y = "Frequency"
    ) +
    theme_minimal()
}
```


for 1000:
```{r}
plot_histogram(train_filtered_1000m_data, "1000m Data", "number_of_persons_injured", "lightblue")
```

for 2000:

```{r}
plot_histogram(train_filtered_2000m_data, "2000m Data", "number_of_persons_injured", "lightgreen")
```
Upon analyzing the distribution of the number_of_persons_injured variable, we observed that the majority of cases had a value of 0, meaning no persons were injured. The remaining values, where injuries were recorded, represent only a small fraction of the data.

Given this heavily skewed distribution, it becomes challenging to draw meaningful insights from the raw number_of_persons_injured variable. Therefore, to simplify the analysis and focus on whether an incident involving injuries occurred, we created a new binary feature called is_persons_injured.

This new feature is defined as follows:
1 indicates that one or more persons were injured during the event.
0 indicates that no injuries occurred.
By converting the variable into a binary form, we can more effectively assess the impact of injury-related incidents on Uber demand and analyze correlations between crashes, safety events, and ride activity without the noise introduced by the original, highly skewed distribution.

```{r}
# Function to create is_persons_injured feature
create_is_persons_injured_feature <- function(df) {
  
  # Create the new binary feature is_persons_injured
  df <- df %>%
    mutate(is_persons_injured = ifelse(number_of_persons_injured > 0, 1, 0))
  
  # Remove the original number_of_persons_injured column
  df <- df %>%
    select(-number_of_persons_injured)
  
  return(df)
}
```

----------------

```{r}
# Function to plot average Uber demand for each crash level
plot_avg_pickups_per_crash <- function(df, data_name, color) {
  
  # Summarize average pickups for each crash level
  crash_summary <- df %>%
    group_by(number_of_crashes) %>%
    summarise(avg_pickups = mean(number_of_pickups, na.rm = TRUE))
  
  # Plot
  ggplot(crash_summary, aes(x = factor(number_of_crashes), y = avg_pickups)) +
    geom_bar(stat = "identity", fill = color, color = "black") +
    labs(
      title = paste("Average Uber Demand per Crash Count -", data_name),
      x = "Number of Crashes",
      y = "Average Uber Pickups"
    ) +
    theme_minimal()
}

```

```{r}
plot_avg_pickups_per_crash(train_filtered_1000m_data, "1000m Data", "lightblue")
```


```{r}
plot_avg_pickups_per_crash(train_filtered_2000m_data, "2000m Data", "lightgreen")
```

From these charts, we observe that in the 1000m radius, Uber demand increases with the number of crashes, suggesting people might prefer Uber in areas affected by traffic incidents. In the 2000m radius, demand remains consistently high across a broader range of crash counts, indicating a steady preference for Uber even in areas with frequent incidents, likely due to varied transportation needs within the larger area.

It makes sense that as the number of Uber trips increases, the likelihood of accidents also rises, since more vehicles on the road increase congestion and collision risks. 

Additionally, the 2000m data shows more accidents overall due to its larger coverage area, capturing more incidents within its expanded radius.


##### Events

```{r}
# Function to create a box plot to show the influence of event clusters on Uber demand with ordered levels
event_cluster_box_plot <- function(data, dataset_name, color_high, color_mid, color_low) {
  
  # Classify days based on total event counts into low, moderate, and high clusters
  data <- data %>%
    group_by(day = as.Date(time_interval)) %>%
    summarise(
      total_events = sum(event_count, na.rm = TRUE),
      total_pickups = sum(number_of_pickups, na.rm = TRUE)
    ) %>%
    mutate(
      event_cluster = case_when(
        total_events <= quantile(total_events, 0.33) ~ "Low",
        total_events <= quantile(total_events, 0.66) ~ "Moderate",
        TRUE ~ "High"
      ),
      event_cluster = factor(event_cluster, levels = c("High", "Moderate", "Low"))  # Set factor order
    )
  
  # Create the box plot
  ggplot(data, aes(x = event_cluster, y = total_pickups, fill = event_cluster)) +
    geom_boxplot() +
    scale_fill_manual(values = c("High" = color_high, "Moderate" = color_mid, "Low" = color_low)) +
    labs(
      title = paste("Uber Pickups by Event Cluster -", dataset_name),
      x = "Event Cluster",
      y = "Total Uber Pickups"
    ) +
    theme_minimal() +
    theme(legend.position = "none")  # Remove legend as it's unnecessary here
}

```
for 1000m:
```{r}
event_cluster_box_plot(train_filtered_1000m_data, "1000m Data", "red", "yellow", "blue")
```

for 2000m:
```{r}
event_cluster_box_plot(train_filtered_2000m_data, "2000m Data", "red", "yellow", "blue")
```
The total number of events in New York City encompasses a wide variety of activities, from large gatherings to smaller, local events. Within the 1000-meter radius of the Empire State Building, the impact of these events on Uber demand appears minimal, as seen in the similar levels of pickups across "Low," "Moderate," and "High" event clusters. This is likely because many events in the city may not directly impact the immediate area around the Empire State. As a result, the data does not show a significant increase in Uber pickups with higher event counts.

In contrast, within the larger 2000-meter radius, which covers a broader part of New York City, there is a clearer pattern. Higher event clusters ("High") show increased Uber demand, suggesting that events across this wider area have a more noticeable influence on transportation needs. This supports the decision to retain the total_events feature in the 2000-meter dataset, as it provides valuable insight into demand variations, while removing it from the 1000-meter dataset where its impact is negligible.


```{r}
# Function to remove the event_count feature from the 1000m dataset
remove_total_events_1000m <- function(df) {
  
  # Check if the event_count column exists in the dataframe
  if ("event_count" %in% colnames(df)) {
    
    # Remove the event_count column
    df <- df %>%
      select(-event_count)
  
  return(df)  # Return the updated dataframe
  }
}
```


##### Feels like

```{r}
# Function to define thresholds, print them, and plot behavior for feels_like with four groups
# and show the percentage of time spent in each range in the legend
plot_feels_like_vs_pickups <- function(data, dataset_name) {
  # Calculate range for feels_like
  feels_like_range <- max(data$feels_like, na.rm = TRUE) - min(data$feels_like, na.rm = TRUE)
  
  # Define thresholds for high, mid-high, mid-low, and low feels_like
  low_feels_like_threshold <- min(data$feels_like, na.rm = TRUE)
  mid_low_feels_like_threshold <- low_feels_like_threshold + feels_like_range / 4
  mid_high_feels_like_threshold <- low_feels_like_threshold + feels_like_range / 2
  high_feels_like_threshold <- low_feels_like_threshold + 3 * (feels_like_range / 4)
  
  # Print the thresholds
  cat("Feels Like Range:\n")
  cat("Low: ", low_feels_like_threshold, "-", mid_low_feels_like_threshold, "\n")
  cat("Mid-Low: ", mid_low_feels_like_threshold, "-", mid_high_feels_like_threshold, "\n")
  cat("Mid-High: ", mid_high_feels_like_threshold, "-", high_feels_like_threshold, "\n")
  cat("High: ", high_feels_like_threshold, "-", max(data$feels_like, na.rm = TRUE), "\n\n")
  
  # Categorize data into four groups based on feels_like
  data <- data %>%
    mutate(
      feels_like_group = case_when(
        feels_like < mid_low_feels_like_threshold ~ paste("Low: ", round(low_feels_like_threshold, 1), "-", round(mid_low_feels_like_threshold, 1)),
        feels_like < mid_high_feels_like_threshold ~ paste("Mid-Low: ", round(mid_low_feels_like_threshold, 1), "-", round(mid_high_feels_like_threshold, 1)),
        feels_like < high_feels_like_threshold ~ paste("Mid-High: ", round(mid_high_feels_like_threshold, 1), "-", round(high_feels_like_threshold, 1)),
        TRUE ~ paste("High: ", round(high_feels_like_threshold, 1), "-", round(max(data$feels_like, na.rm = TRUE), 1))
      )
    )
  
  # Summarize pickups by group and calculate percentage
  group_summary <- data %>%
    group_by(feels_like_group) %>%
    summarise(
      total_pickups = sum(number_of_pickups, na.rm = TRUE),
      time_count = n()  # Count the number of times in each group
    ) %>%
    mutate(
      pickup_percentage = (total_pickups / sum(total_pickups)) * 100,
      time_percentage = (time_count / sum(time_count)) * 100  # Percentage of time in each range
    )
  
  # Arrange levels in order from High to Low
  group_summary$feels_like_group <- factor(group_summary$feels_like_group, 
                                           levels = c(
                                             paste("High: ", round(high_feels_like_threshold, 1), "-", round(max(data$feels_like, na.rm = TRUE), 1)),
                                             paste("Mid-High: ", round(mid_high_feels_like_threshold, 1), "-", round(high_feels_like_threshold, 1)),
                                             paste("Mid-Low: ", round(mid_low_feels_like_threshold, 1), "-", round(mid_high_feels_like_threshold, 1)),
                                             paste("Low: ", round(low_feels_like_threshold, 1), "-", round(mid_low_feels_like_threshold, 1))
                                           ))

  # Define colors dynamically based on levels
  colors <- c("red", "orange", "yellow", "blue")
  names(colors) <- levels(group_summary$feels_like_group)
  
  # Create custom legend labels with time percentages
  legend_labels <- paste0(levels(group_summary$feels_like_group), " (Time: ", sprintf("%.1f%%", group_summary$time_percentage), ")")
  
  # Create a bar plot with Uber demand percentages and custom legend for time percentages
  ggplot(group_summary) +
    geom_bar(aes(x = feels_like_group, y = pickup_percentage, fill = feels_like_group), 
             stat = "identity", color = "black", position = position_dodge(width = 0.7), width = 0.5) +
    geom_text(aes(x = feels_like_group, y = pickup_percentage, 
                  label = sprintf("%.1f%%", pickup_percentage)), vjust = -0.5, size = 5, color = "black") +
    labs(
      title = paste("Uber Demand by Feels Like Ranges -", dataset_name),
      x = "Feels Like Temperature Range",
      y = "Percentage of Total Uber Pickups (%)",
      fill = "Feels Like Temperature Ranges\n(includes Time Distribution)"
    ) +
    scale_fill_manual(values = colors, labels = legend_labels) +
    scale_y_continuous(limits = c(0, 100), labels = scales::percent_format(scale = 1)) +  # Ensure y-axis ranges from 0 to 100
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 30, hjust = 1, size = 12),  # Adjust x-axis labels for better readability
      axis.text.y = element_text(size = 12),
      axis.title = element_text(size = 14),
      plot.title = element_text(size = 16, face = "bold")
    )
}
```

```{r}
plot_feels_like_vs_pickups(train_filtered_1000m_data, "1000m Data")
```

```{r}
plot_feels_like_vs_pickups(train_filtered_2000m_data, "2000m Data")
```

The "feels like" temperature significantly influences Uber demand in this dataset, which primarily covers the warmer summer months. The majority of Uber pickups are concentrated in "feels like" temperature ranges that are moderately high (18.8 - 27.9Â°C), as seen by the high percentage of pickups in this range for both the 1000m and 2000m datasets. This pattern suggests that when the weather is comfortable or mildly warm, people are more likely to go out, leading to higher Uber demand.

On the other hand, in extreme "feels like" conditionsâ€”both high and lowâ€”the demand for Uber drops. In particularly hot weather (27.9 - 37Â°C), fewer people may choose to leave their homes, possibly preferring to work remotely or avoid outings to stay comfortable. Similarly, in cooler conditions (0.5 - 9.7Â°C), there may also be reduced activity, though this temperature range is less represented in the summer data.

Overall, these insights reflect the tendency of people to stay home when the weather is uncomfortable, whether itâ€™s too hot or relatively cool. Thus, "feels like" temperature becomes a significant factor in Uber demand, with moderate, comfortable weather encouraging more travel, while extreme temperatures have the opposite effect.

##### EDA Conclusions

Here we will run the functions related to the changes we would like to make in the feature due to the conclusions we reached in the eda phase:

For 1000:
```{r}
categorical_features_1000 <- categorical_features
numerical_features_1000 <- numerical_features
```

Conclusions implemented on the 1000m train + test:
```{r}
# Function to update train_filtered_1000m_data with specified transformations
update_data_1000m <- function(df) {
  
  # Add is_holiday feature and remove original holiday columns
  df <- create_is_holiday_feature(df)
  
  # Add is_persons_injured feature and remove original injury column
  df <- create_is_persons_injured_feature(df)
  
  # Remove the total_events feature
  df <- remove_total_events_1000m(df)
  
  return(df)  # Return the updated dataframe
}
```

```{r}
train_filtered_1000m_data <- update_data_1000m(train_filtered_1000m_data)
test_fixed_data_1000 <- update_data_1000m(test_fixed_data)
```

```{r}
# Features to remove
cat_remove <- c("memorial_day", "independence_day", "labor_day")
categorical_features_1000 <- setdiff(categorical_features_1000, cat_remove)

num_remove <- c("event_count", "number_of_persons_injured")
numerical_features_1000 <- setdiff(numerical_features_1000, num_remove)

# Features to add
cat_add <- c("is_holiday", "is_persons_injured")
categorical_features_1000 <- unique(c(categorical_features_1000, cat_add))
```

```{r}
categorical_features_1000
```

```{r}
numerical_features_1000
```

```{r}
train_filtered_1000m_data
```

```{r}
test_fixed_data_1000
```


For 2000:
```{r}
categorical_features_2000 <- categorical_features
numerical_features_2000 <- numerical_features
```

Conclusions implemented on the 1000m train + test:
```{r}
# Function to update train_filtered_2000m_data with specified transformations
update_data_2000m <- function(df) {
  
  # Add is_holiday feature and remove original holiday columns
  df <- create_is_holiday_feature(df)
  
  # Add is_persons_injured feature and remove original injury column
  df <- create_is_persons_injured_feature(df)
  
  return(df)  # Return the updated dataframe
}
```

```{r}
train_filtered_2000m_data <- update_data_2000m(train_filtered_2000m_data)
test_fixed_data_2000 <- update_data_2000m(test_fixed_data)
```

```{r}
# Features to remove
categorical_features_2000 <- setdiff(categorical_features_2000, cat_remove)

numerical_features_2000 <- setdiff(numerical_features_2000, "number_of_persons_injured")

# Features to add
cat_add <- c("is_holiday", "is_persons_injured")
categorical_features_2000 <- unique(c(categorical_features_2000, cat_add))
```

```{r}
categorical_features_2000
```

```{r}
numerical_features_2000
```

```{r}
train_filtered_2000m_data
```

```{r}
test_fixed_data_2000
```

Lets save to CSVs:
```{r}
write.csv(train_filtered_1000m_data, "final_train_filtered_1000m_data.csv")
write.csv(test_fixed_data_1000, "final_test_fixed_data_1000.csv")

write.csv(train_filtered_2000m_data, "final_train_filtered_2000m_data.csv")
write.csv(test_fixed_data_2000, "final_test_fixed_data_2000.csv")
```

#### Spliting the data
By splitting the data into training and validation sets, we can train our model on the training set and assess its performance on the validation set. This helps us understand how well our model is likely to perform on new, unseen data. Additionally, inorder to avoid data leaking, and understanding that onehotencoder is based on fitting and transforming, from now on will work with test_train split.

Since our project is based on time intervals (time series), it is essential to respect the chronological order of the data. Our training data spans 6 months, 
*TODO decide on a correct test_train split*

```{r}
range(train_filtered_1000m_data$time_interval)
```

```{r}
range(train_filtered_2000m_data$time_interval)
```

```{r split to val and train}

```

## Part C: Forecast for the Future

### Model 1 - Full Data
Final Data for Modeling:
Since the test data doesn't provide exact pickup coordinates (latitude and longitude), but all pickups are known to occur within a 1000-meter radius from the Empire State Building, it's essential to ensure that the final training dataset only includes pickups within this same 1000-meter radius. Therefore, we must filter the training data accordingly. Additionally, because the test data lacks latitude, longitude, and distance features, it's important to remove these features from the final train
ing dataset before modeling. This step ensures consistency between the training and test datasets, enabling the model to make accurate predictions.

```{r model_1_full_data}
```

### Model 2 - Filtered Data
1. The Problem:
The training data includes Uber pickups within a 2000-meter radius, while the test data represents pickups within a 1000-meter radius around the Empire State Building. This difference in radius creates a problem because the model might learn patterns from a wider area (2000m) that do not apply to the narrower, more localized 1000m area required for the predictions in the test data.

2. Clustering Solution:
Since the test data doesn't include specific locations (latitude, longitude), and the cluster based on location from the test data is not present in the training data, we need a non-location-based clustering approach.

```{r model_2_filtered_data}
```

## Conclusions
```{r conclusion}
```
```{r}

```

